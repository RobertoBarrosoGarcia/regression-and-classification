{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del Dataset\n",
    "Base de datos de reconocimiento de actividad humana (HAR) construida a partir de las grabaciones de 30 sujetos que realizan actividades de la vida diaria (ADL) mientras llevan un teléfono inteligente montado en la cintura con sensores de inercia integrados.\n",
    "\n",
    "Cada persona realizó seis actividades (Caminar, Caminar cuesta arriba, Caminar cuesta abajo, Sentado, De pie, Acostado) con un teléfono inteligente (Samsung Galaxy S II) en la cintura. Usando su acelerómetro y giroscopio integrados, se captura la aceleración lineal 3-axial y la velocidad angular 3-axial a una tasa constante de 50Hz.\n",
    "\n",
    "### Información de los atributos\n",
    "\n",
    "Entrada:\n",
    "\n",
    "* 561 atributos que indican en forma de vector las variables que captan los sensores:\n",
    "    * Aceleración triaxial del acelerómetro (aceleración total) y la aceleración corporal estimada.\n",
    "    * Velocidad angular triaxial del giroscopio.\n",
    "\n",
    "Salida (nuestro objetivo en la clasificación):\n",
    "\n",
    "* Class: Variable categórica mapeada a numérica:\n",
    "    * 1: Caminando\n",
    "    * 2: Caminando cuesta arriba\n",
    "    * 3: Caminando cuesta abajo\n",
    "    * 4: Sentado\n",
    "    * 5: De pie\n",
    "    * 6: Acostado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T00:11:56.338306Z",
     "iopub.status.busy": "2020-09-23T00:11:56.337617Z",
     "iopub.status.idle": "2020-09-23T00:12:03.150200Z",
     "shell.execute_reply": "2020-09-23T00:12:03.150753Z"
    },
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import kerastuner as kt\n",
    "\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga de datos\n",
    "Obtenemos el set de datos al que vamos a aplicar la clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T00:12:03.156630Z",
     "iopub.status.busy": "2020-09-23T00:12:03.154908Z",
     "iopub.status.idle": "2020-09-23T00:12:04.600741Z",
     "shell.execute_reply": "2020-09-23T00:12:04.600139Z"
    },
    "id": "7MqDQO0KCaWS",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V553</th>\n",
       "      <th>V554</th>\n",
       "      <th>V555</th>\n",
       "      <th>V556</th>\n",
       "      <th>V557</th>\n",
       "      <th>V558</th>\n",
       "      <th>V559</th>\n",
       "      <th>V560</th>\n",
       "      <th>V561</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989303</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>0.310155</td>\n",
       "      <td>-0.053391</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>-0.287866</td>\n",
       "      <td>-0.140589</td>\n",
       "      <td>-0.215088</td>\n",
       "      <td>-0.356083</td>\n",
       "      <td>-0.148775</td>\n",
       "      <td>-0.232057</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376278</td>\n",
       "      <td>-0.750809</td>\n",
       "      <td>-0.337422</td>\n",
       "      <td>0.346295</td>\n",
       "      <td>0.884904</td>\n",
       "      <td>-0.698885</td>\n",
       "      <td>-0.651732</td>\n",
       "      <td>0.274627</td>\n",
       "      <td>0.184784</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10295</th>\n",
       "      <td>0.363385</td>\n",
       "      <td>-0.039214</td>\n",
       "      <td>-0.105915</td>\n",
       "      <td>-0.305388</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>-0.196373</td>\n",
       "      <td>-0.373540</td>\n",
       "      <td>-0.030036</td>\n",
       "      <td>-0.270237</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320418</td>\n",
       "      <td>-0.700274</td>\n",
       "      <td>-0.736701</td>\n",
       "      <td>-0.372889</td>\n",
       "      <td>-0.657421</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>-0.655181</td>\n",
       "      <td>0.273578</td>\n",
       "      <td>0.182412</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10296</th>\n",
       "      <td>0.349966</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>-0.115788</td>\n",
       "      <td>-0.329638</td>\n",
       "      <td>-0.042143</td>\n",
       "      <td>-0.250181</td>\n",
       "      <td>-0.388017</td>\n",
       "      <td>-0.133257</td>\n",
       "      <td>-0.347029</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118854</td>\n",
       "      <td>-0.467179</td>\n",
       "      <td>-0.181560</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>0.696664</td>\n",
       "      <td>0.363139</td>\n",
       "      <td>-0.655357</td>\n",
       "      <td>0.274479</td>\n",
       "      <td>0.181184</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10297</th>\n",
       "      <td>0.237594</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>-0.096499</td>\n",
       "      <td>-0.323114</td>\n",
       "      <td>-0.229775</td>\n",
       "      <td>-0.207574</td>\n",
       "      <td>-0.392380</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>-0.289477</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205445</td>\n",
       "      <td>-0.617737</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>-0.819188</td>\n",
       "      <td>0.929294</td>\n",
       "      <td>-0.008398</td>\n",
       "      <td>-0.659719</td>\n",
       "      <td>0.264782</td>\n",
       "      <td>0.187563</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>0.153627</td>\n",
       "      <td>-0.018437</td>\n",
       "      <td>-0.137018</td>\n",
       "      <td>-0.330046</td>\n",
       "      <td>-0.195253</td>\n",
       "      <td>-0.164339</td>\n",
       "      <td>-0.430974</td>\n",
       "      <td>-0.218295</td>\n",
       "      <td>-0.229933</td>\n",
       "      <td>-0.111527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072237</td>\n",
       "      <td>-0.436940</td>\n",
       "      <td>0.598808</td>\n",
       "      <td>-0.287951</td>\n",
       "      <td>0.876030</td>\n",
       "      <td>-0.024965</td>\n",
       "      <td>-0.660080</td>\n",
       "      <td>0.263936</td>\n",
       "      <td>0.188103</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10299 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
       "1      0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
       "2      0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
       "3      0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
       "4      0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10294  0.310155 -0.053391 -0.099109 -0.287866 -0.140589 -0.215088 -0.356083   \n",
       "10295  0.363385 -0.039214 -0.105915 -0.305388  0.028148 -0.196373 -0.373540   \n",
       "10296  0.349966  0.030077 -0.115788 -0.329638 -0.042143 -0.250181 -0.388017   \n",
       "10297  0.237594  0.018467 -0.096499 -0.323114 -0.229775 -0.207574 -0.392380   \n",
       "10298  0.153627 -0.018437 -0.137018 -0.330046 -0.195253 -0.164339 -0.430974   \n",
       "\n",
       "             V8        V9       V10  ...      V553      V554      V555  \\\n",
       "0     -0.983185 -0.923527 -0.934724  ... -0.298676 -0.710304 -0.112754   \n",
       "1     -0.974914 -0.957686 -0.943068  ... -0.595051 -0.861499  0.053477   \n",
       "2     -0.963668 -0.977469 -0.938692  ... -0.390748 -0.760104 -0.118559   \n",
       "3     -0.982750 -0.989303 -0.938692  ... -0.117290 -0.482845 -0.036788   \n",
       "4     -0.979672 -0.990441 -0.942469  ... -0.351471 -0.699205  0.123320   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "10294 -0.148775 -0.232057  0.185361  ... -0.376278 -0.750809 -0.337422   \n",
       "10295 -0.030036 -0.270237  0.185361  ... -0.320418 -0.700274 -0.736701   \n",
       "10296 -0.133257 -0.347029  0.007471  ... -0.118854 -0.467179 -0.181560   \n",
       "10297 -0.279610 -0.289477  0.007471  ... -0.205445 -0.617737  0.444558   \n",
       "10298 -0.218295 -0.229933 -0.111527  ... -0.072237 -0.436940  0.598808   \n",
       "\n",
       "           V556      V557      V558      V559      V560      V561  Class  \n",
       "0      0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627      5  \n",
       "1     -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317      5  \n",
       "2      0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118      5  \n",
       "3     -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663      5  \n",
       "4      0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892      5  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "10294  0.346295  0.884904 -0.698885 -0.651732  0.274627  0.184784      2  \n",
       "10295 -0.372889 -0.657421  0.322549 -0.655181  0.273578  0.182412      2  \n",
       "10296  0.088574  0.696664  0.363139 -0.655357  0.274479  0.181184      2  \n",
       "10297 -0.819188  0.929294 -0.008398 -0.659719  0.264782  0.187563      2  \n",
       "10298 -0.287951  0.876030 -0.024965 -0.660080  0.263936  0.188103      2  \n",
       "\n",
       "[10299 rows x 562 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://drive.google.com/file/d/1UrhszIqhL0Pu9Nme2su4DveONaeGRFfe/view?usp=sharing'\n",
    "url2='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos si tiene valores desconocidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1       0\n",
       "V2       0\n",
       "V3       0\n",
       "V4       0\n",
       "V5       0\n",
       "        ..\n",
       "V558     0\n",
       "V559     0\n",
       "V560     0\n",
       "V561     0\n",
       "Class    0\n",
       "Length: 562, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No existen valores desconocidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadísticas de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V553</th>\n",
       "      <th>V554</th>\n",
       "      <th>V555</th>\n",
       "      <th>V556</th>\n",
       "      <th>V557</th>\n",
       "      <th>V558</th>\n",
       "      <th>V559</th>\n",
       "      <th>V560</th>\n",
       "      <th>V561</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274347</td>\n",
       "      <td>-0.017743</td>\n",
       "      <td>-0.108925</td>\n",
       "      <td>-0.607784</td>\n",
       "      <td>-0.510191</td>\n",
       "      <td>-0.613064</td>\n",
       "      <td>-0.633593</td>\n",
       "      <td>-0.525697</td>\n",
       "      <td>-0.614989</td>\n",
       "      <td>-0.466732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298592</td>\n",
       "      <td>-0.617700</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>-0.009219</td>\n",
       "      <td>-0.496522</td>\n",
       "      <td>0.063255</td>\n",
       "      <td>-0.054284</td>\n",
       "      <td>3.624624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.067628</td>\n",
       "      <td>0.037128</td>\n",
       "      <td>0.053033</td>\n",
       "      <td>0.438694</td>\n",
       "      <td>0.500240</td>\n",
       "      <td>0.403657</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.399034</td>\n",
       "      <td>0.538707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320199</td>\n",
       "      <td>0.308796</td>\n",
       "      <td>0.336591</td>\n",
       "      <td>0.447364</td>\n",
       "      <td>0.616189</td>\n",
       "      <td>0.484770</td>\n",
       "      <td>0.511158</td>\n",
       "      <td>0.305468</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>1.743695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262624</td>\n",
       "      <td>-0.024903</td>\n",
       "      <td>-0.121019</td>\n",
       "      <td>-0.992360</td>\n",
       "      <td>-0.976990</td>\n",
       "      <td>-0.979137</td>\n",
       "      <td>-0.993294</td>\n",
       "      <td>-0.977017</td>\n",
       "      <td>-0.979064</td>\n",
       "      <td>-0.935788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.536174</td>\n",
       "      <td>-0.841848</td>\n",
       "      <td>-0.124694</td>\n",
       "      <td>-0.287031</td>\n",
       "      <td>-0.493108</td>\n",
       "      <td>-0.389041</td>\n",
       "      <td>-0.817287</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>-0.131880</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277174</td>\n",
       "      <td>-0.017162</td>\n",
       "      <td>-0.108596</td>\n",
       "      <td>-0.943030</td>\n",
       "      <td>-0.835032</td>\n",
       "      <td>-0.850773</td>\n",
       "      <td>-0.948244</td>\n",
       "      <td>-0.843670</td>\n",
       "      <td>-0.845068</td>\n",
       "      <td>-0.874825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335160</td>\n",
       "      <td>-0.703402</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.017192</td>\n",
       "      <td>-0.007186</td>\n",
       "      <td>-0.715631</td>\n",
       "      <td>0.182028</td>\n",
       "      <td>-0.003882</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288354</td>\n",
       "      <td>-0.010625</td>\n",
       "      <td>-0.097589</td>\n",
       "      <td>-0.250293</td>\n",
       "      <td>-0.057336</td>\n",
       "      <td>-0.278737</td>\n",
       "      <td>-0.302033</td>\n",
       "      <td>-0.087404</td>\n",
       "      <td>-0.288149</td>\n",
       "      <td>-0.014641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113167</td>\n",
       "      <td>-0.487981</td>\n",
       "      <td>0.149006</td>\n",
       "      <td>0.291490</td>\n",
       "      <td>0.536137</td>\n",
       "      <td>0.365996</td>\n",
       "      <td>-0.521503</td>\n",
       "      <td>0.250791</td>\n",
       "      <td>0.102970</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 V1            V2            V3            V4            V5  \\\n",
       "count  10299.000000  10299.000000  10299.000000  10299.000000  10299.000000   \n",
       "mean       0.274347     -0.017743     -0.108925     -0.607784     -0.510191   \n",
       "std        0.067628      0.037128      0.053033      0.438694      0.500240   \n",
       "min       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%        0.262624     -0.024903     -0.121019     -0.992360     -0.976990   \n",
       "50%        0.277174     -0.017162     -0.108596     -0.943030     -0.835032   \n",
       "75%        0.288354     -0.010625     -0.097589     -0.250293     -0.057336   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 V6            V7            V8            V9           V10  \\\n",
       "count  10299.000000  10299.000000  10299.000000  10299.000000  10299.000000   \n",
       "mean      -0.613064     -0.633593     -0.525697     -0.614989     -0.466732   \n",
       "std        0.403657      0.413333      0.484201      0.399034      0.538707   \n",
       "min       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%       -0.979137     -0.993294     -0.977017     -0.979064     -0.935788   \n",
       "50%       -0.850773     -0.948244     -0.843670     -0.845068     -0.874825   \n",
       "75%       -0.278737     -0.302033     -0.087404     -0.288149     -0.014641   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...          V553          V554          V555          V556  \\\n",
       "count  ...  10299.000000  10299.000000  10299.000000  10299.000000   \n",
       "mean   ...     -0.298592     -0.617700      0.007705      0.002648   \n",
       "std    ...      0.320199      0.308796      0.336591      0.447364   \n",
       "min    ...     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%    ...     -0.536174     -0.841848     -0.124694     -0.287031   \n",
       "50%    ...     -0.335160     -0.703402      0.008146      0.007668   \n",
       "75%    ...     -0.113167     -0.487981      0.149006      0.291490   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "               V557          V558          V559          V560          V561  \\\n",
       "count  10299.000000  10299.000000  10299.000000  10299.000000  10299.000000   \n",
       "mean       0.017683     -0.009219     -0.496522      0.063255     -0.054284   \n",
       "std        0.616189      0.484770      0.511158      0.305468      0.268898   \n",
       "min       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%       -0.493108     -0.389041     -0.817287      0.002151     -0.131880   \n",
       "50%        0.017192     -0.007186     -0.715631      0.182028     -0.003882   \n",
       "75%        0.536137      0.365996     -0.521503      0.250791      0.102970   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              Class  \n",
       "count  10299.000000  \n",
       "mean       3.624624  \n",
       "std        1.743695  \n",
       "min        1.000000  \n",
       "25%        2.000000  \n",
       "50%        4.000000  \n",
       "75%        5.000000  \n",
       "max        6.000000  \n",
       "\n",
       "[8 rows x 562 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance de datos\n",
    "Se debe comprobar como de balanceados se encuentran nuestros datos, esto consiste en comprobar el número de observaciones que tenemos por cada clase de salida. Muchos algoritmos son muy sensibles a estas diferencias de proporción, por eso es importante contar con un set de datos balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    1944\n",
       "5    1906\n",
       "4    1777\n",
       "1    1722\n",
       "2    1544\n",
       "3    1406\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAi6klEQVR4nO3deZgdVbnv8e+PMEPClAZDQhLAwD2AEqBBlAOiqIDKpILBI5NIkCcoKPcoeLwKxxvlXgUFEZQ5yBADEYwKAiKDKBgTDAQCSEiCaRJIM5kIGkl4zx+1NhTN7q5K957S/fs8Tz1dtWp6q4f9dq21apUiAjMzs56s0ewAzMys9TlZmJlZIScLMzMr5GRhZmaFnCzMzKyQk4WZmRVysrDVjqS9JT1eo2PtK6mjFsfqQwzHSrq3xsccLSkkrVnL4/ZW0TVKukXSMb08ds1+H6x7LfGLZPUjaQGwBbAyV7xdRCxqTkR9FxG/A7ZvdhxWOxFxYNltJQUwJiLmpn39+9AAvrMYGA6KiA1z05sSRav892lmrcvJYoBKVRQTJD0BPJHKPipplqSXJP1B0jtz228l6WeSOiU9L+mCVH6mpKtz272p+kPSRpIuk7RY0tOS/q+kQWndsZLulfRdSS9Kmi/pwNyxNpV0haRFaf1NqfxNVUeSTpf0pKRlkuZIOqyH615P0pXpeHOA3bus31LS1HSd8yV9oeBY50h6StLf0rWsl9ZdL+mZVH6PpB1z+20maZqkpZKmA9t2Oe55kham9TMl7d2bGLpsd5ykR9P3aJ6kE3Prhkr6Zfq5vyDpd5LW6MX3YyNJV6Vtn5L0tcpx3thEP0hxPiZpv9yKuyR9Nrf8mRTvi5JulTQqld+TNnlQ0t8lfTL/+5B+F26o8v08P3c909J1zpV0QnfXY11EhKd+PAELgA9UKQ/gdmBTYD1gV2AJ8C5gEHBM2nedtPwg8D1gA2Bd4N/Tcc4Ers4dd3Q69ppp+Sbgx2m/zYHpwIlp3bHAq8AJ6RwnAYsApfW/An4KbAKsBbw3le8LdOTOeTiwJdk/P58EXgaGdfP9OBv4XbrurYCHK8dK+88Evg6sDWwDzAP27+ZYPwTuAoan+N8DrJPWfQYYnL5/3wdm5fabDExJ35OdgKeBe3PrPw1sRlZNfBrwDLDuqsRQ5efwEbKkJOC9wCvArmndt4Efpe/xWsDeabtV/X5cBfw8Xfdo4C/A8bmf9Qrgi+kcnwT+Bmya1t8FfDbNHwrMBf4tfQ++Bvyhy+/u23PLr/8+AKPStQ1Jy4OAxcCeaflu4EKy3+GxQCewX7P/TleHqekBeKrzDzj7wP878FKabkrlAbw/t91FwDe77Pt4+mB5d/qjWrPK8c+km2RB1layHFgvt/5I4M40fywwN7du/bTv24BhwGvAJlXO+fqHQzfXPAs4pJt184ADcsvjcx807wL+2mX7M4ArqhxnDeAfwM4lfgYbp+vaKH14vQr8r9z6b5FLFlX2f7HaeXqKgS7Josr6m4BT0vx/k33Iv73LNqvy/RiUftY75MpOBO7K/axf/0cglU0Hjkrzd/FGsriFlGRy1/kKMCr3u1s1WaTle4Gj0/wHgSfT/FZkbXeDc9t+G7iyln9z/XVyNdTAcGhEbJymQ3PlC3Pzo4DTUlXES5JeIvvj2jJ9fSoiVqzieUeR/Re5OHfMH5PdYVQ8U5mJiFfS7IbpnC9ExItFJ5F0tN6oPnuJ7L/1od1sviVvvu6nusS7ZZfvwVfJkl5XQ8n+O32ySjyDJJ2dqsaWkiXsyj5tZIm0uxiQdFqqgvlbimGjbq6n2xiqxHSgpPtT9ctLwIdzx/wO2X/yt6UqqtNT+ap+P9buci1Pkd3xVDwd6RM6t37LKscaBZyXO+cLZHc6w6tsW821ZP+UAHwqLZPO9UJELOshRuuGk8XAlv/DXQhMzCWVjSNi/Yi4Lq0bqeoN4S+T3RFUvK3LMZcDQ3PHHBIRO1JsIbCppI172ijVZV8CnAxsFhEbk1UtqZtdFpMlooqRXc45v8v3YHBEfLjKcZ4D/kmX9obkU8AhwAfIPuhHV8Ilu0Nb0V0MqX3iK8ARZHdVG5NV11S7np5ieJ2kdYCpwHeBLdIxb64cMyKWRcRpEbENcBDwpdSesKrfj1fJPujz1/V0bnm4JHVZX61X3kKyqsr8edeLiD/0dJ051wP7ShoBHMYbyWIR2e/U4B5itG44WVjFJcDnJL1LmQ0kfST9YU0n+5A9O5WvK2mvtN8sYB9JIyVtRFZNAUBELAZuA86RNETSGpK2lfTeomDSvrcAF0raRNJakvapsukGZEmvE7KGXLI7i+5MAc5IxxwBfD63bjqwVNJXlDUcD5K0k6Tdux4kIl4DLgfOTY2mgyS9O30wDyZLks+TJdJv5fZbCfwMOFPS+pJ2IGsfqhhMlkw6gTUlfR0Y0s33qKcY8tYma8foBFYo60TwocpKZR0b3p4+yJeSVdWsXMXvx8r0vZ0oaXBK4l8Crs5ttjnwhfSzPJysTeLmKpf2I7Kf0Y4pvo3S9hXPkrWfVBURnWTVWleQJbtHU/lC4A/At9Pv8DuB44FrujuWvcHJwgCIiBlkDc0XkNWRzyWrZ658EBwEvB34K9BB1kBJRNxO1gj9EFlj6C+7HPposg+rOem4N5C1R5RxFNl/q4+RNb6fWiXuOcA5wH1kHyLvAH7fwzHPIqt6mE+WyH6SO1blOsem9c8Bl5LdHVTzv4HZwJ/Iqkr+H9nf1FXpHE+TXff9XfY7mayq7RngSrIPtYpbyZLkX9Ix/smbq6zKxvC6VO3yBbIP8xfJ7nym5TYZA/yGrG3rPuDCiLirF9+Pz5Pdac4jaze4liyZVfwxnes5YCLwiYh4vutBIuLGdB2TUzXew0D+OYwzgUmpmuqIbmK5luzO7tou5UeS3ektAm4EvpF+h61ApdeJmVnTpC6xl0bEVc2OxarznYWZNZWk9cmqleY3OxbrnpOFmTWNpM3JquPuJqu6shblaigzMyvkOwszMyvUbweQGzp0aIwePbrZYZiZrVZmzpz5XES0dS3vt8li9OjRzJgxo9lhmJmtViQ9Va28btVQykYpvTMNW/CIpFNS+aaSbpf0RPq6SW6fM9JIkI9L2j9Xvpuk2Wnd+V2eAjUzszqrZ5vFCuC0iPg3YE9gQnpa9XTgjogYA9yRlknrxgE7AgeQPbk7KB3rIrIB38ak6YA6xm1mZl3ULVlExOKIeCDNLwMeJRuw6xBgUtpsEtlwxKTyyRGxPCLmkz1BvIekYWTDDd+XBiG7KrePmZk1QEN6Q0kaDexC9rj/Fmncn8r4P5URSIfz5mENOlLZ8DTftbzaecZLmiFpRmdnZ02vwcxsIKt7spC0IdmIl6dGxNKeNq1SFj2Uv7Uw4uKIaI+I9ra2tzTmm5lZL9U1WUhaiyxRXBMRP0vFz6aqJdLXJam8gzcP2zyCbLCvjjTftdzMzBqknr2hBFwGPBoR5+ZWTeONIZmPIXtDV6V8nKR1JG1N1pA9PVVVLZO0Zzrm0bl9zMysAer5nMVeZENMz5Y0K5V9lewdyFMkHU823PXhABHxiKQpZEM6rwAmpCGSIXs385Vk74q+JU1mZtYg/XZsqPb29vBDeWZmq0bSzIho71reb5/g7q3Rp/+qoedbcPZHGno+M7Pe8ECCZmZWyMnCzMwKOVmYmVkht1kMIG6PMbPe8p2FmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCThZmZlbIycLMzArV8x3cl0taIunhXNlPJc1K04LK61YljZb0j9y6H+X22U3SbElzJZ2f3sNtZmYNVM9RZ68ELgCuqhRExCcr85LOAf6W2/7JiBhb5TgXAeOB+4GbgQPwO7itCo+qa1Y/dbuziIh7gBeqrUt3B0cA1/V0DEnDgCERcV9kLwu/Cji0xqGamVmBZrVZ7A08GxFP5Mq2lvRnSXdL2juVDQc6ctt0pLKqJI2XNEPSjM7OztpHbWY2QDUrWRzJm+8qFgMjI2IX4EvAtZKGANXaJ6K7g0bExRHRHhHtbW1tNQ3YzGwga/ib8iStCXwM2K1SFhHLgeVpfqakJ4HtyO4kRuR2HwEsaly0ZmYGzXmt6geAxyLi9eolSW3ACxGxUtI2wBhgXkS8IGmZpD2BPwJHAz9oQsxmTecGfGumenadvQ64D9heUoek49Oqcby1YXsf4CFJDwI3AJ+LiErj+EnApcBc4EncE8rMrOHqdmcREUd2U35slbKpwNRutp8B7FTT4MzMbJX4CW4zMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMytUmCwk7SVpgzT/aUnnShpV/9DMzKxVlLmzuAh4RdLOwJeBp8i9/c7MzPq/MsliRXpL3SHAeRFxHjC4vmGZmVkrKTOQ4DJJZwBHAXtLGgSsVd+wzMyslZS5s/gk2YuJPhMRz5C91vQ7dY3KzMxaSmGySAliKrBOKnoOuLGeQZmZWWsp0xvqBLIXEv04FQ0HbqpjTGZm1mLKtFlMAPYge60pEfGEpM3rGpWZDTh+bWxrK9NmsTwi/lVZkLQmEPULyczMWk2ZZHG3pK8C60n6IHA98IuinSRdLmmJpIdzZWdKelrSrDR9OLfuDElzJT0uaf9c+W6SZqd150vSql2imZn1VZlkcTrQCcwGTgRuBr5WYr8rgQOqlH8vIsam6WYASTsA44Ad0z4Xpi66kD0UOB4Yk6ZqxzQzszoqbLOIiNeAS9JUWkTcI2l0yc0PASZHxHJgvqS5wB6SFgBDIuI+AElXAYcCt6xKLGZm1jdlx4a6XdJfJM2TNF/SvD6c82RJD6Vqqk1S2XBgYW6bjlQ2PM13Le8u1vGSZkia0dnZ2YcQzcwsr0w11GXAucC/A7sD7elrb1wEbAuMBRYD56Tyau0Q0UN5VRFxcUS0R0R7W1tbL0M0M7OuynSd/VtE1KTaJyKercxLugT4ZVrsALbKbToCWJTKR1QpNzOzBipzZ3GnpO9IerekXStTb04maVhu8TCg0lNqGjBO0jqStiZryJ4eEYvJxqbaM/WCOhr4eW/ObWZmvVfmzuJd6Wt7riyA9/e0k6TrgH2BoZI6gG8A+0oam/ZfQNa7ioh4RNIUYA6wApgQESvToU4i61m1HlnDthu3zcwarExvqPf15sARcWSV4st62H4iMLFK+Qxgp97EYGZmtVGmN9RG6e14M9J0jqSNGhGcmZm1hjJtFpcDy4Aj0rQUuKKeQZmZWWsp02axbUR8PLd8lqRZdYrHzMxaUJk7i39I+vfKgqS9gH/ULyQzM2s1Ze4sTgImpXYKAS8Ax9YzKDOz/mZ1H4K9TG+oWcDOkoak5aU1jcDMzFpet8lC0qcj4mpJX+pSDkBEnFvn2MzMrEX0dGexQfo6uBGBmJlZ6+o2WUTEj9PXsxoXjpmZtaIyD+VNkrRxbnkTSZfXNSozM2spZbrOvjMiXqosRMSLwC51i8jMzFpOmWSxRu4lRUjalHJdbs3MrJ8o86F/DvAHSTek5cOpMuCfmZn1X2Wes7hK0kzgfWQP5X0sIubUPTIzM2sZpaqT0vsmOoF1ASSNjIi/1jUyMzNrGWV6Qx0s6QlgPnA32UuL/AIiM7MBpEwD9zeBPYG/RMTWwH7A7+salZmZtZQyyeLViHierFfUGhFxJzC2aCdJl0taIunhXNl3JD0m6SFJN1ae35A0WtI/JM1K049y++wmabakuZLOV2W8ETMza5gyyeIlSRsC9wDXSDqP7D3ZRa4EDuhSdjuwU0S8E/gLcEZu3ZMRMTZNn8uVXwSMB8akqesxzcyszsoki0OAV4AvAr8GngQOKtopIu4hG848X3ZbRFQSzf3AiJ6OIWkYMCQi7ouIAK4CDi0Rs5mZ1VCZZDEe2DIiVkTEpIg4P1VL9dVneHND+daS/izpbkl7p7LhQEdum45UZmZmDVSm6+wQ4FZJLwCTgRsi4tm+nFTSf5FVZV2TihYDIyPieUm7ATdJ2pHsuY6uoofjjidLbowcObIvIZqZWU7hnUVEnBUROwITgC2BuyX9prcnlHQM8FHgP1LVEhGxvHK3EhEzyaq6tiO7k8hXVY0AFvUQ68UR0R4R7W1tbb0N0czMuihTDVWxBHgGeB7YvDcnk3QA8BXg4Ih4JVfeJmlQmt+GrCF7XkQsBpZJ2jP1gjoa+Hlvzm1mZr1X5qG8kyTdBdwBDAVOSL2Ziva7DrgP2F5Sh6TjgQvIXqZ0e5cusvsAD0l6ELgB+FxEVBrHTwIuBeaS3XH4gUAzswYr02YxEjg1vYu7tIg4skrxZd1sOxWY2s26GcBOq3JuMzOrrR7vLCStARy0qonCzMz6lx6TRUS8BjwoyV2LzMwGsDLVUMOARyRNB16uFEbEwXWLyszMWkqZZHFW3aMwM7OWVublR3dLGgWMiYjfSFofGFT/0MzMrFWU6Tp7All31h+nouHATXWMyczMWkyZh/ImAHsBSwEi4gl6+VCemZmtnsoki+UR8a/KgqQ16WF8JjMz63/KJIu7JX0VWE/SB4HrgV/UNywzM2slZZLF6UAnMBs4EbgZ+Fo9gzIzs9ZSpjfUa8AlwCWSNgVGVEaLNTOzgaFMb6i7JA1JiWIWcIWkc+semZmZtYwy1VAbRcRS4GPAFRGxG/CB+oZlZmatpEyyWDO9C/sI4Jd1jsfMzFpQmWTx38CtwJMR8af0cqIn6huWmZm1kjIN3NeTdZetLM8DPl7PoMzMrLWUaeDeRtIvJHVKWiLp55K2bkRwZmbWGspUQ10LTCEbqnxLsruMyfUMyszMWkuZZKGI+ElErEjT1ZQY7kPS5elO5OFc2aaSbpf0RPq6SW7dGZLmSnpc0v658t0kzU7rzpekVb1IMzPrm26TRfpg3xS4U9LpkkZLGiXpy8CvShz7SuCALmWnA3dExBjgjrSMpB2AccCOaZ8LJVWGQb8IGA+MSVPXY5qZWZ311MA9k+wOovKf/Im5dQF8s6cDR8Q9kkZ3KT4E2DfNTwLuAr6SyidHxHJgvqS5wB6SFgBDIuI+AElXAYcCt/R0bjMzq61uk0VE1KMRe4uIWJyOv1hSZajz4cD9ue06Utmrab5reVWSxpPdhTBypF8bbmZWK2V6Q60l6QuSbkjTyZLWqnEc1dohoofyqiLi4ohoj4j2tra2mgVnZjbQlWngvgjYDbgwTbulst54Nj0NTvq6JJV3AFvlthsBLErlI6qUm5lZA5VJFrtHxDER8ds0HQfs3svzTQOOSfPHAD/PlY+TtE56hmMMMD1VWS2TtGfqBXV0bh8zM2uQwie4gZWSto2IJyF7SA9YWbSTpOvIGrOHSuoAvgGcDUyRdDzwV+BwgIh4RNIUYA6wApgQEZVznETWs2o9soZtN26bmTVYmWTxn2TdZ+eRtSGMAo4r2ikijuxm1X7dbD8RmFilfAawU4k4zcysTsqMDXWHpDHA9mTJ4rHUxdXMzAaIMncWpOTwUJ1jMTOzFlWmgdvMzAY4JwszMytU5qE8Sfq0pK+n5ZGS9qh/aGZm1irK3FlcCLwbqPRuWgb8sG4RmZlZyynTwP2uiNhV0p8BIuJFSWvXOS4zM2shZe4sXk3DhQeApDbgtbpGZWZmLaVMsjgfuBHYXNJE4F7gW3WNyszMWkqZh/KukTST7MlrAYdGxKN1j8zMzFpGt8kivSWvYglwXX5dRLxQz8DMzKx1lH1T3kjgxTS/MdkggPV4OZKZmbWgbtssImLriNgGuBU4KCKGRsRmwEeBnzUqQDMza76y77O4ubIQEbcA761fSGZm1mrKPGfxnKSvAVeTVUt9Gni+rlGZmVlLKXNncSTQRtZ99sY03927KszMrB8q03X2BeCUBsRiZmYtyqPOmplZoYYnC0nbS5qVm5ZKOlXSmZKezpV/OLfPGZLmSnpc0v6NjtnMbKAr9aa8WoqIx4GxAGnMqafJ2kKOA74XEd/Nby9pB2AcsCOwJfAbSdtFxMpGxm1mNpCVeZ/FCEk3SuqU9KykqZJG1Oj8+wFPRsRTPWxzCDA5IpZHxHxgLuD3aZiZNVCZaqgrgGnAMGA48ItUVgvjyA0jApws6SFJl0vaJJUNBxbmtulIZW8habykGZJmdHZ21ihEMzMrkyzaIuKKiFiRpivJus/2SXonxsHA9anoImBbsiqqxcA5lU2r7B7VjhkRF0dEe0S0t7X1OUQzM0vKJIvn0mtVB6WpVg/lHQg8EBHPAkTEsxGxMiJeAy7hjaqmDmCr3H4jgEU1OL+ZmZVUJll8BjgCeIbsP/5PpLK+OpI3j2Q7LLfuMODhND8NGCdpHUlbA2OA6TU4v5mZldRjb6jUW+lbEXFwLU8qaX3gg8CJueL/L2ksWRXTgsq6iHhE0hRgDrACmOCeUGZmjdVjsoiIlZLaJK0dEf+q1Ukj4hVgsy5lR/Ww/URgYq3Ob2Zmq6bMcxYLgN9Lmga8XCmMiHPrFZSZmbWWMsliUZrWAAbXNxwzM2tFZQYSPAtA0gYR8XLR9mZm1v+UeYL73ZLmAI+m5Z0lXVj3yMzMrGWU6Tr7fWB/0rMVEfEgsE8dYzIzsxZTatTZiFjYpchdV83MBpAyDdwLJb0HiDRExxdIVVJmZjYwlLmz+BwwgWzwvg6ysZsm1DEmMzNrMWV6Qz0H/EcDYjEzsxZVmCzSeEyfB0bnt6/1ECBmZta6yrRZ3ARcRvYei9fqGo2ZmbWkMsninxFxft0jMTOzllUmWZwn6RvAbcDySmFEPFC3qMzMrKWUSRbvAI4C3s8b1VCRls3MbAAokywOA7ap5RDlZma2einznMWDwMZ1jsPMzFpYmTuLLYDHJP2JN7dZuOusmdkAUSZZfKPWJ5W0AFhGNsbUiohol7Qp8FOy5zkWAEdExItp+zOA49P2X4iIW2sdk5mZda/ME9x31+nc70tPh1ecDtwREWdLOj0tf0XSDsA4YEdgS+A3krbze7jNzBqnapuFpPVz88skLU3TPyWtlLS0DrEcAkxK85OAQ3PlkyNieUTMB+YCe9Th/GZm1o3uGriPlfRfABExOCKGpGld4OPABX08bwC3SZopaXwq2yIiFqdzLgY2T+XDgfwQ6R2pzMzMGqRqsoiIC4GnJB1dZd1N9P0Zi70iYlfgQGCCpJ5epqRqIVbdUBovaYakGZ2dnX0M0czMKrpts4iIqwEkfSxXvAbQTjcf1mVFxKL0dYmkG8mqlZ6VNCwiFksaBixJm3cAW+V2HwEs6ua4FwMXA7S3t/cpRjMze0OZ5ywOyk37k/ViOqS3J5S0gaTBlXngQ8DDwDTgmLTZMcDP0/w0YJykddIIuGOA6b09v5mZrboyvaGOq/E5twBulFQ5/7UR8ev0HMcUSccDfwUOT+d/RNIUYA6wApjgnlBmZo3VbbKQ9PUe9ouI+GZvThgR84Cdq5Q/D+zXzT4TgYm9OZ+ZmfVdT3cWL1cp24Ds4bjNgF4lCzMzW/301MB9TmU+tTGcAhwHTAbO6W4/MzPrf3pss0hDcHyJ7B3ck4BdK0NwmJnZwNFTm8V3gI+RdUV9R0T8vWFRmZlZS+mp6+xpZGMxfQ1YlBvyY1mdhvswM7MW1VObRZlnMMzMbABwQjAzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr5GRhZmaFnCzMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZoYYnC0lbSbpT0qOSHpF0Sio/U9LTkmal6cO5fc6QNFfS45L2b3TMZmYDXY8vP6qTFcBpEfFAegPfTEm3p3Xfi4jv5jeWtAMwDtiRbMj030jaLiJWNjRqM7MBrOF3FhGxOCIeSPPLgEeB4T3scggwOSKWR8R8YC6wR/0jNTOziqa2WUgaDewC/DEVnSzpIUmXS9oklQ0HFuZ266Cb5CJpvKQZkmZ0dnbWK2wzswGnaclC0obAVODUiFgKXARsC4wFFgPnVDatsntUO2ZEXBwR7RHR3tbWVvugzcwGqKYkC0lrkSWKayLiZwAR8WxErIyI14BLeKOqqQPYKrf7CGBRI+M1MxvomtEbSsBlwKMRcW6ufFhus8OAh9P8NGCcpHUkbQ2MAaY3Kl4zM2tOb6i9gKOA2ZJmpbKvAkdKGktWxbQAOBEgIh6RNAWYQ9aTaoJ7QpmZNVbDk0VE3Ev1doibe9hnIjCxbkGZmVmP/AS3mZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCq02ykHSApMclzZV0erPjMTMbSFaLZCFpEPBD4EBgB+BISTs0Nyozs4FjtUgWwB7A3IiYFxH/AiYDhzQ5JjOzAUMR0ewYCkn6BHBARHw2LR8FvCsiTu6y3XhgfFrcHni8gWEOBZ5r4PkaqT9fG/j6Vne+vtoaFRFtXQvXbGAAfaEqZW/JchFxMXBx/cN5K0kzIqK9Geeut/58beDrW935+hpjdamG6gC2yi2PABY1KRYzswFndUkWfwLGSNpa0trAOGBak2MyMxswVotqqIhYIelk4FZgEHB5RDzS5LC6akr1V4P052sDX9/qztfXAKtFA7eZmTXX6lINZWZmTeRkYWZmhZws+kjSxpJukPSYpEclvbvZMdWSpAWSZkuaJWlGs+OpNUmDJP1Z0i+bHUutSbpc0hJJDzc7llqTtJWkO9Pf3COSTml2TLUkaV1J0yU9mK7vrKbH5DaLvpE0CfhdRFyaemqtHxEvNTmsmpG0AGiPiH750JOkLwHtwJCI+Giz46klSfsAfweuioidmh1PLUkaBgyLiAckDQZmAodGxJwmh1YTkgRsEBF/l7QWcC9wSkTc36yYfGfRB5KGAPsAlwFExL/6U6Lo7ySNAD4CXNrsWOohIu4BXmh2HPUQEYsj4oE0vwx4FBje3KhqJzJ/T4trpamp/9k7WfTNNkAncEWqyrhU0gbNDqrGArhN0sw0nEp/8n3gy8BrTY7D+kDSaGAX4I9NDqWmUhXpLGAJcHtENPX6nCz6Zk1gV+CiiNgFeBnob8On7xURu5KN+DshVW2s9iR9FFgSETObHYv1nqQNganAqRGxtNnx1FJErIyIsWQjVuwhqalViU4WfdMBdOQy/g1kyaPfiIhF6esS4EayEYD7g72Ag1ObzGTg/ZKubm5ItipSXf5U4JqI+Fmz46mXVLV9F3BAM+NwsuiDiHgGWChp+1S0H9AvGtgAJG2QGg9J1WsfAvpFz5qIOCMiRkTEaLLhY34bEZ9uclhWUmoAvgx4NCLObXY8tSapTdLGaX494APAY82MabUY7qPFfR64JvWEmgcc1+R4amkL4Mbs75I1gWsj4tfNDcnKknQdsC8wVFIH8I2IuKy5UdXMXsBRwOxUrw/w1Yi4uXkh1dQwYFJ68dsawJSIaGr3bnedNTOzQq6GMjOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGHWR5LeJmmypCclzZF0s6Tt+uNorzZw+TkLsz5ID4fdCEyKiHGpbCzZMypm/YbvLMz65n3AqxHxo0pBRMwCFlaWJY2W9DtJD6TpPal8mKR70rtCHpa0dxo87sq0PFvSFxt+RWZV+M7CrG92InuXQk+WAB+MiH9KGgNcR/YOjU8Bt0bExPSk7vrAWGB45f0TlSEfzJrNycKs/tYCLkjVUyuB7VL5n4DL04B4N0XELEnzgG0k/QD4FXBbMwI268rVUGZ98wiwW8E2XwSeBXYmu6NYG15/OdE+wNPATyQdHREvpu3uAibQT1/MZKsfJwuzvvktsI6kEyoFknYHRuW22QhYHBGvkQ1+NyhtN4rsnRqXkI2guqukocAaETEV+D/0syHvbfXlaiizPoiIkHQY8H1JpwP/BBYAp+Y2uxCYKulw4E6yl2RBNiLsf0p6lexd2UeTvRr0CkmVf+TOqPc1mJXhUWfNzKyQq6HMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr9D/m0QFKDZWcUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(df['Class'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Frecuencia de cada clase objetivo\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Número de observaciones\");\n",
    "count_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general el número de observaciones de cada clase en nuestro set de datos está bastante balanceado, además no sobresale ninguno en exceso. Más adelante veremos si tenemos algún problema en la clasificación en este sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de entrenamiento y datos de prueba\n",
    "\n",
    "Después de varías iteraciones vemos que la mejor opción es dividir los datos en un 80% de entrenamiento y 20% de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values\n",
    "X = array[:,0:561]\n",
    "Y = array[:,561]\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(\n",
    "X, Y, test_size=0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación mediante algoritmo clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los algoritmos\n",
    "Añadimos varios algoritmos conocidos para ver cual de todos funciona mejor con nuestro set de datos. El mejor será el elegido para nuestra comparación con la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append((\"RFC\", RandomForestClassifier(n_jobs=2, random_state=0)))\n",
    "models.append((\"KNN\", KNeighborsClassifier()))\n",
    "models.append((\"NB\", GaussianNB()))\n",
    "models.append((\"SVC\", SVC(gamma='auto')))\n",
    "models.append((\"MLP\", MLPClassifier(activation=\"relu\", alpha=1e05, batch_size=\"auto\",beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e08, hidden_layer_sizes=(3, 3),learning_rate=\"constant\", learning_rate_init=0.001, max_iter=200, momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True, solver=\"lbfgs\", tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES6uQoLKCaWr"
   },
   "source": [
    "### Evaluamos cada modelo por turnos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T00:12:04.662721Z",
     "iopub.status.busy": "2020-09-23T00:12:04.645719Z",
     "iopub.status.idle": "2020-09-23T00:12:04.845607Z",
     "shell.execute_reply": "2020-09-23T00:12:04.846037Z"
    },
    "id": "m4VEw8Ud9Quh",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC: 0.976697 (0.004907)\n",
      "KNN: 0.962739 (0.004971)\n",
      "NB: 0.725453 (0.026185)\n",
      "SVC: 0.952301 (0.007418)\n",
      "MLP: 0.192014 (0.000479)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de algoritmos gráficamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.boxplot(results, labels=names)\n",
    "pyplot.title('Comparación de algoritmos')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo que mejor resultados obtiene es Random Forest Classification (RFC), por tanto es el que vamos a utilizar contra el basado en redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasificador: RFC \n",
      "precision media:  0.976332 (0.003735)\n",
      "Precision de una prediccion:  0.9791262135922331\n",
      "\n",
      "Matriz de confusion: \n",
      " [[348   3   4   0   0   0]\n",
      " [  0 302   2   0   0   0]\n",
      " [  1   6 277   0   0   0]\n",
      " [  0   0   0 348  18   0]\n",
      " [  0   0   0   9 380   0]\n",
      " [  0   0   0   0   0 362]]\n",
      "\n",
      "Reporte de clasificacion: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.98      0.99       355\n",
      "         2.0       0.97      0.99      0.98       304\n",
      "         3.0       0.98      0.98      0.98       284\n",
      "         4.0       0.97      0.95      0.96       366\n",
      "         5.0       0.95      0.98      0.97       389\n",
      "         6.0       1.00      1.00      1.00       362\n",
      "\n",
      "    accuracy                           0.98      2060\n",
      "   macro avg       0.98      0.98      0.98      2060\n",
      "weighted avg       0.98      0.98      0.98      2060\n",
      "\n",
      "RFC FP: 43\n",
      "RFC FN: 43\n",
      "RFC TP: 2017\n",
      "RFC TN: 10257\n",
      "RFC TPR: 0.979456380132166\n",
      "RFC TNR: 0.9958041608086244\n",
      "RFC FPR: 0.004195839191375506\n",
      "RFC PPV: 0.9794263559367088\n",
      "RFC NPV: 0.9958068909528562\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el algoritmo\n",
    "model = RandomForestClassifier()\n",
    "name = 'RFC'\n",
    "\n",
    "# Utilizaremos una validación cruzada estratificada de 10 veces (k-fold) para estimar la precisión del modelo\n",
    "k_fold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# Resultados de la precisión\n",
    "cv_results = cross_val_score(model, X_train, Y_train, cv=k_fold, scoring='accuracy')\n",
    "results.append(cv_results)\n",
    "print('\\nClasificador: %s %s %f (%f)' % (name, '\\nprecision media: ', cv_results.mean(), cv_results.std()))\n",
    "# Ejecutamos modelo\n",
    "model.fit(X_train, Y_train)\n",
    "# Hacemos una predicción con nuestro modelo\n",
    "predictions = model.predict(X_validation)\n",
    "# Evaluamos las predicciones y obtenemos cual ha sido la precisión\n",
    "print('Precision de una prediccion: ', accuracy_score(Y_validation, predictions))\n",
    "# Matriz de confusión\n",
    "print('\\nMatriz de confusion: \\n', confusion_matrix(Y_validation, predictions))\n",
    "# El reporte de clasificación que muestra un desglose de cada clase por precisión, recuerdo,  puntuación f1 y apoyo\n",
    "print('\\nReporte de clasificacion: \\n', classification_report(Y_validation, predictions))\n",
    "\n",
    "# Valores para la comparativa\n",
    "# Falsos positivos\n",
    "FP = confusion_matrix(Y_validation, predictions).sum(axis=0) - np.diag(\n",
    "    confusion_matrix(Y_validation, predictions))\n",
    "# Falsos negativos\n",
    "FN = confusion_matrix(Y_validation, predictions).sum(axis=1) - np.diag(\n",
    "    confusion_matrix(Y_validation, predictions))\n",
    "# Verdaderos positivos\n",
    "TP = np.diag(confusion_matrix(Y_validation, predictions))\n",
    "# Verdaderos negativos\n",
    "TN = confusion_matrix(Y_validation, predictions).sum() - (FP + FN + TP)\n",
    "# True positive rate (sensitivity)\n",
    "TPR = TP / (TP + FN)\n",
    "# True negative rate (specify)\n",
    "TNR = TN / (TN + FP)\n",
    "# False positive rate\n",
    "FPR = FP / (FP + TN)\n",
    "# Positive predicted value\n",
    "PPV = TP / (TP + FP)\n",
    "# Negative predicted value\n",
    "NPV = TN / (TN + FN)\n",
    "\n",
    "# Mostrar Resultados\n",
    "print(name + \" FP: \" + str(FP.sum()))\n",
    "print(name + \" FN: \" + str(FN.sum()))\n",
    "print(name + \" TP: \" + str(TP.sum()))\n",
    "print(name + \" TN: \" + str(TN.sum()))\n",
    "print(name + \" TPR: \" + str(TPR.mean()))\n",
    "print(name + \" TNR: \" + str(TNR.mean()))\n",
    "print(name + \" FPR: \" + str(FPR.mean()))\n",
    "print(name + \" PPV: \" + str(PPV.mean()))\n",
    "print(name + \" NPV: \" + str(NPV.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones del modelo\n",
    "Tenemos una precisón media de 0.976, lo cual es un valor muy alto e indica que nuestro modelo tiene un gran acierto al predecir pertenencia de clase.\n",
    "\n",
    "Para las clases '1', '4' y '6' (caminando, sentado y acostado) tenemos una alta precisión y un menor recall, esto podría indicarnos que el modelo es peor detectando la clase pero cuando lo hace es muy confiable, de igual forma los 2 valores son casi 1 lo cual indica que casi alcanza la perfección.\n",
    "\n",
    "En las clases '2', '3' y '5' (caminando cuesta arriba, caminando cuesta abajo y estando de pie) tenemos un recall mayor que la precisión, esto nos indicaría que se detecta muy bien la clase pero no es del todo confiable.\n",
    "\n",
    "Si observamos el valor de f1-score que es dado por la media harmonía de precisión y recall, tenemos que la clase que mejor se detecta es la '6' (estar de pie), y la que peor es la '4' (estar sentado). Aunque en general los valores son casi 1 por lo que se puede afirmar que el modelo maneja perfectamente todas las clases.\n",
    "\n",
    "Además, no obtenemos alto valor de precisión en la clase Mayoritaria y un bajo recall en la clase Minoritaria, por lo que tambien podemos decir que se trata de un set de datos muy bien balanceado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación basado en redes neuronales mediante TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59veuiEZCaW4"
   },
   "source": [
    "## Construir el Modelo\n",
    "\n",
    "Construir la red neuronal requiere configurar las capas del modelo y luego compilar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gxg1XGm0eOBy"
   },
   "source": [
    "### Configurar las Capas\n",
    "\n",
    "Los bloques de construccion basicos de una red neuronal son las *capas* o *layers*. Las capas extraen representaciones de el set de datos que se les alimentan. Con suerte, estas representaciones son considerables para el problema que estamos solucionando.\n",
    "\n",
    "La mayoria de aprendizaje profundo consiste de unir capas sencillas. \n",
    "La mayoria de las capas como `tf.keras.layers.Dense`, tienen parametros que son aprendidos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gut8A_7rCaW6"
   },
   "source": [
    "### Compilar el modelo\n",
    "* *Optimizer* — Esto es como el modelo se actualiza basado en el set de datos que ve y la funcion de perdida.\n",
    "* *Loss* —Esto mide que tan exacto es el modelo durante el entrenamiento. Quiere minimizar esta funcion para dirigir el modelo en la direccion adecuada.\n",
    "* *Metrics* — Se usan para monitorear los pasos de entrenamiento y de pruebas. Usamos Accuracy para saber la proporción de las clases que son correctamente clasificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_dim = dim))\n",
    "\n",
    "    # Valor óptimo de la densidad de las capas entre 32 y 512\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    print(hp_units)\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    # Valor óptimo del 'learning rate' (0.01, 0.001, o 0.0001)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de ajuste 'hyperband' utiliza la asignación de recursos adaptativa y la detención anticipada para converger rápidamente en un modelo de alto rendimiento. Entrena una gran cantidad de modelos durante algunas épocas y lleva solo la mitad de los modelos con mejor rendimiento a la siguiente ronda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project my_dir\\intro_to_kt\\oracle.json\n",
      "32\n",
      "INFO:tensorflow:Reloading Tuner from my_dir\\intro_to_kt\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos 'stop_early' para detener el entrenamiento antes de alcanzar un cierto valor en la pérdida de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los 'hiperparámetros' óptimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 96 and the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, Y_train, epochs=200, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKF6uW-BCaW-"
   },
   "source": [
    "## Entrenar el Modelo\n",
    "\n",
    "Se entrenan los datos mediante el metodo fit que ajusta el modelo con los datos de entrenamiento que estan en los arrays X_train e Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "Epoch 1/200\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.4683 - accuracy: 0.8120 - val_loss: 0.3238 - val_accuracy: 0.8647\n",
      "Epoch 2/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1624 - accuracy: 0.9370 - val_loss: 0.2254 - val_accuracy: 0.9041\n",
      "Epoch 3/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1578 - accuracy: 0.9386 - val_loss: 0.3171 - val_accuracy: 0.9029\n",
      "Epoch 4/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1353 - accuracy: 0.9504 - val_loss: 0.0767 - val_accuracy: 0.9672\n",
      "Epoch 5/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1030 - accuracy: 0.9612 - val_loss: 0.0748 - val_accuracy: 0.9684\n",
      "Epoch 6/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0876 - accuracy: 0.9645 - val_loss: 0.1014 - val_accuracy: 0.9600\n",
      "Epoch 7/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0940 - accuracy: 0.9630 - val_loss: 0.0863 - val_accuracy: 0.9715\n",
      "Epoch 8/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0955 - accuracy: 0.9642 - val_loss: 0.1356 - val_accuracy: 0.9527\n",
      "Epoch 9/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0857 - accuracy: 0.9681 - val_loss: 0.1250 - val_accuracy: 0.9539\n",
      "Epoch 10/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1059 - accuracy: 0.9609 - val_loss: 0.0877 - val_accuracy: 0.9666\n",
      "Epoch 11/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0953 - accuracy: 0.9677 - val_loss: 0.0672 - val_accuracy: 0.9733\n",
      "Epoch 12/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0959 - accuracy: 0.9634 - val_loss: 0.3580 - val_accuracy: 0.8732\n",
      "Epoch 13/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0766 - accuracy: 0.9700 - val_loss: 0.0636 - val_accuracy: 0.9763\n",
      "Epoch 14/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0879 - accuracy: 0.9681 - val_loss: 0.0708 - val_accuracy: 0.9715\n",
      "Epoch 15/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0860 - accuracy: 0.9674 - val_loss: 0.0666 - val_accuracy: 0.9733\n",
      "Epoch 16/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0675 - accuracy: 0.9744 - val_loss: 0.0904 - val_accuracy: 0.9600\n",
      "Epoch 17/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0705 - accuracy: 0.9730 - val_loss: 0.0675 - val_accuracy: 0.9757\n",
      "Epoch 18/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0715 - accuracy: 0.9718 - val_loss: 0.0795 - val_accuracy: 0.9727\n",
      "Epoch 19/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0688 - accuracy: 0.9763 - val_loss: 0.0835 - val_accuracy: 0.9666\n",
      "Epoch 20/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0763 - accuracy: 0.9704 - val_loss: 0.1054 - val_accuracy: 0.9600\n",
      "Epoch 21/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0877 - accuracy: 0.9677 - val_loss: 0.0937 - val_accuracy: 0.9666\n",
      "Epoch 22/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0667 - accuracy: 0.9750 - val_loss: 0.0569 - val_accuracy: 0.9782\n",
      "Epoch 23/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0639 - accuracy: 0.9741 - val_loss: 0.0624 - val_accuracy: 0.9739\n",
      "Epoch 24/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0650 - accuracy: 0.9759 - val_loss: 0.0545 - val_accuracy: 0.9806\n",
      "Epoch 25/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0597 - accuracy: 0.9782 - val_loss: 0.0545 - val_accuracy: 0.9782\n",
      "Epoch 26/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0432 - accuracy: 0.9822 - val_loss: 0.0565 - val_accuracy: 0.9812\n",
      "Epoch 27/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0501 - accuracy: 0.9797 - val_loss: 0.0835 - val_accuracy: 0.9672\n",
      "Epoch 28/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0472 - accuracy: 0.9798 - val_loss: 0.0649 - val_accuracy: 0.9769\n",
      "Epoch 29/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0956 - accuracy: 0.9657 - val_loss: 0.0610 - val_accuracy: 0.9769\n",
      "Epoch 30/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0585 - accuracy: 0.9766 - val_loss: 0.0569 - val_accuracy: 0.9782\n",
      "Epoch 31/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0538 - accuracy: 0.9785 - val_loss: 0.0578 - val_accuracy: 0.9751\n",
      "Epoch 32/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9783 - val_loss: 0.0545 - val_accuracy: 0.9769\n",
      "Epoch 33/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0527 - accuracy: 0.9797 - val_loss: 0.0670 - val_accuracy: 0.9739\n",
      "Epoch 34/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0513 - accuracy: 0.9792 - val_loss: 0.0628 - val_accuracy: 0.9794\n",
      "Epoch 35/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1368 - accuracy: 0.9592 - val_loss: 0.2025 - val_accuracy: 0.9132\n",
      "Epoch 36/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1085 - accuracy: 0.9621 - val_loss: 0.0685 - val_accuracy: 0.9739\n",
      "Epoch 37/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0894 - accuracy: 0.9686 - val_loss: 0.0606 - val_accuracy: 0.9800\n",
      "Epoch 38/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0653 - accuracy: 0.9738 - val_loss: 0.1104 - val_accuracy: 0.9460\n",
      "Epoch 39/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0586 - accuracy: 0.9754 - val_loss: 0.0656 - val_accuracy: 0.9794\n",
      "Epoch 40/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0627 - accuracy: 0.9759 - val_loss: 0.0658 - val_accuracy: 0.9788\n",
      "Epoch 41/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0529 - accuracy: 0.9795 - val_loss: 0.0615 - val_accuracy: 0.9763\n",
      "Epoch 42/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0527 - accuracy: 0.9780 - val_loss: 0.0603 - val_accuracy: 0.9757\n",
      "Epoch 43/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0437 - accuracy: 0.9838 - val_loss: 0.0974 - val_accuracy: 0.9618\n",
      "Epoch 44/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0553 - accuracy: 0.9798 - val_loss: 0.0862 - val_accuracy: 0.9703\n",
      "Epoch 45/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0799 - accuracy: 0.9734 - val_loss: 0.1722 - val_accuracy: 0.9466\n",
      "Epoch 46/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0500 - accuracy: 0.9812 - val_loss: 0.0737 - val_accuracy: 0.9763\n",
      "Epoch 47/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0550 - accuracy: 0.9798 - val_loss: 0.0806 - val_accuracy: 0.9733\n",
      "Epoch 48/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0420 - accuracy: 0.9822 - val_loss: 0.0610 - val_accuracy: 0.9775\n",
      "Epoch 49/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0451 - accuracy: 0.9822 - val_loss: 0.0635 - val_accuracy: 0.9763\n",
      "Epoch 50/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0416 - accuracy: 0.9842 - val_loss: 0.0588 - val_accuracy: 0.9794\n",
      "Epoch 51/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0440 - accuracy: 0.9821 - val_loss: 0.0600 - val_accuracy: 0.9788\n",
      "Epoch 52/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0409 - accuracy: 0.9853 - val_loss: 0.0564 - val_accuracy: 0.9788\n",
      "Epoch 53/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0469 - accuracy: 0.9822 - val_loss: 0.0574 - val_accuracy: 0.9788\n",
      "Epoch 54/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0545 - accuracy: 0.9794 - val_loss: 0.0717 - val_accuracy: 0.9745\n",
      "Epoch 55/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0499 - accuracy: 0.9789 - val_loss: 0.0939 - val_accuracy: 0.9630\n",
      "Epoch 56/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0492 - accuracy: 0.9803 - val_loss: 0.0733 - val_accuracy: 0.9703\n",
      "Epoch 57/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0432 - accuracy: 0.9821 - val_loss: 0.0651 - val_accuracy: 0.9751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0501 - accuracy: 0.9812 - val_loss: 0.0573 - val_accuracy: 0.9775\n",
      "Epoch 59/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0447 - accuracy: 0.9816 - val_loss: 0.0553 - val_accuracy: 0.9775\n",
      "Epoch 60/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0458 - accuracy: 0.9818 - val_loss: 0.0776 - val_accuracy: 0.9703\n",
      "Epoch 61/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0414 - accuracy: 0.9832 - val_loss: 0.0562 - val_accuracy: 0.9794\n",
      "Epoch 62/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0400 - accuracy: 0.9845 - val_loss: 0.0520 - val_accuracy: 0.9800\n",
      "Epoch 63/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0489 - accuracy: 0.9812 - val_loss: 0.0616 - val_accuracy: 0.9782\n",
      "Epoch 64/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1139 - accuracy: 0.9724 - val_loss: 0.1054 - val_accuracy: 0.9630\n",
      "Epoch 65/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0545 - accuracy: 0.9803 - val_loss: 0.0661 - val_accuracy: 0.9757\n",
      "Epoch 66/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0533 - accuracy: 0.9804 - val_loss: 0.0663 - val_accuracy: 0.9751\n",
      "Epoch 67/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0514 - accuracy: 0.9780 - val_loss: 0.1010 - val_accuracy: 0.9508\n",
      "Epoch 68/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0457 - accuracy: 0.9832 - val_loss: 0.0699 - val_accuracy: 0.9782\n",
      "Epoch 69/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0580 - accuracy: 0.9775 - val_loss: 0.0640 - val_accuracy: 0.9763\n",
      "Epoch 70/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0445 - accuracy: 0.9835 - val_loss: 0.0723 - val_accuracy: 0.9733\n",
      "Epoch 71/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0396 - accuracy: 0.9833 - val_loss: 0.0592 - val_accuracy: 0.9800\n",
      "Epoch 72/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0526 - accuracy: 0.9810 - val_loss: 0.0591 - val_accuracy: 0.9782\n",
      "Epoch 73/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0350 - accuracy: 0.9862 - val_loss: 0.0624 - val_accuracy: 0.9800\n",
      "Epoch 74/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0480 - accuracy: 0.9809 - val_loss: 0.0654 - val_accuracy: 0.9800\n",
      "Epoch 75/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0419 - accuracy: 0.9856 - val_loss: 0.0609 - val_accuracy: 0.9800\n",
      "Epoch 76/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0484 - accuracy: 0.9804 - val_loss: 0.1951 - val_accuracy: 0.9053\n",
      "Epoch 77/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0440 - accuracy: 0.9832 - val_loss: 0.0581 - val_accuracy: 0.9812\n",
      "Epoch 78/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0483 - accuracy: 0.9824 - val_loss: 0.0784 - val_accuracy: 0.9715\n",
      "Epoch 79/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0398 - accuracy: 0.9832 - val_loss: 0.0645 - val_accuracy: 0.9812\n",
      "Epoch 80/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0411 - accuracy: 0.9835 - val_loss: 0.0810 - val_accuracy: 0.9691\n",
      "Epoch 81/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0446 - accuracy: 0.9819 - val_loss: 0.0798 - val_accuracy: 0.9733\n",
      "Epoch 82/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0361 - accuracy: 0.9853 - val_loss: 0.0641 - val_accuracy: 0.9812\n",
      "Epoch 83/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0439 - accuracy: 0.9838 - val_loss: 0.0657 - val_accuracy: 0.9794\n",
      "Epoch 84/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0650 - accuracy: 0.9742 - val_loss: 0.0653 - val_accuracy: 0.9775\n",
      "Epoch 85/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0373 - accuracy: 0.9856 - val_loss: 0.0769 - val_accuracy: 0.9733\n",
      "Epoch 86/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0394 - accuracy: 0.9839 - val_loss: 0.0631 - val_accuracy: 0.9800\n",
      "Epoch 87/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0392 - accuracy: 0.9827 - val_loss: 0.0719 - val_accuracy: 0.9763\n",
      "Epoch 88/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0375 - accuracy: 0.9863 - val_loss: 0.0625 - val_accuracy: 0.9806\n",
      "Epoch 89/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0377 - accuracy: 0.9842 - val_loss: 0.0614 - val_accuracy: 0.9800\n",
      "Epoch 90/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0346 - accuracy: 0.9879 - val_loss: 0.0674 - val_accuracy: 0.9794\n",
      "Epoch 91/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0478 - accuracy: 0.9816 - val_loss: 0.0588 - val_accuracy: 0.9818\n",
      "Epoch 92/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0359 - accuracy: 0.9839 - val_loss: 0.0611 - val_accuracy: 0.9806\n",
      "Epoch 93/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0451 - accuracy: 0.9810 - val_loss: 0.0632 - val_accuracy: 0.9782\n",
      "Epoch 94/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0408 - accuracy: 0.9841 - val_loss: 0.0590 - val_accuracy: 0.9812\n",
      "Epoch 95/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0341 - accuracy: 0.9870 - val_loss: 0.0629 - val_accuracy: 0.9812\n",
      "Epoch 96/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9804 - val_loss: 0.0620 - val_accuracy: 0.9836\n",
      "Epoch 97/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0630 - accuracy: 0.9760 - val_loss: 0.6943 - val_accuracy: 0.8720\n",
      "Epoch 98/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1368 - accuracy: 0.9619 - val_loss: 0.1457 - val_accuracy: 0.9660\n",
      "Epoch 99/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1094 - accuracy: 0.9747 - val_loss: 0.0586 - val_accuracy: 0.9806\n",
      "Epoch 100/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0417 - accuracy: 0.9841 - val_loss: 0.0590 - val_accuracy: 0.9818\n",
      "Epoch 101/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0609 - accuracy: 0.9774 - val_loss: 0.0693 - val_accuracy: 0.9715\n",
      "Epoch 102/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0485 - accuracy: 0.9832 - val_loss: 0.3035 - val_accuracy: 0.8968\n",
      "Epoch 103/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0618 - accuracy: 0.9730 - val_loss: 0.1556 - val_accuracy: 0.9618\n",
      "Epoch 104/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0772 - accuracy: 0.9722 - val_loss: 0.1437 - val_accuracy: 0.9527\n",
      "Epoch 105/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1129 - accuracy: 0.9684 - val_loss: 0.0624 - val_accuracy: 0.9782\n",
      "Epoch 106/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0575 - accuracy: 0.9782 - val_loss: 0.0592 - val_accuracy: 0.9800\n",
      "Epoch 107/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0522 - accuracy: 0.9824 - val_loss: 0.0576 - val_accuracy: 0.9800\n",
      "Epoch 108/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0342 - accuracy: 0.9865 - val_loss: 0.0691 - val_accuracy: 0.9775\n",
      "Epoch 109/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0363 - accuracy: 0.9860 - val_loss: 0.0587 - val_accuracy: 0.9824\n",
      "Epoch 110/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0583 - accuracy: 0.9769 - val_loss: 0.2016 - val_accuracy: 0.8574\n",
      "Epoch 111/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0419 - accuracy: 0.9835 - val_loss: 0.0616 - val_accuracy: 0.9788\n",
      "Epoch 112/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0866 - accuracy: 0.9727 - val_loss: 0.1909 - val_accuracy: 0.9448\n",
      "Epoch 113/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0687 - accuracy: 0.9744 - val_loss: 0.0824 - val_accuracy: 0.9636\n",
      "Epoch 114/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0359 - accuracy: 0.9854 - val_loss: 0.0708 - val_accuracy: 0.9745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0400 - accuracy: 0.9853 - val_loss: 0.0579 - val_accuracy: 0.9788\n",
      "Epoch 116/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0339 - accuracy: 0.9859 - val_loss: 0.0545 - val_accuracy: 0.9800\n",
      "Epoch 117/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0398 - accuracy: 0.9859 - val_loss: 0.0568 - val_accuracy: 0.9806\n",
      "Epoch 118/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0548 - accuracy: 0.9800 - val_loss: 0.0602 - val_accuracy: 0.9788\n",
      "Epoch 119/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0349 - accuracy: 0.9874 - val_loss: 0.0571 - val_accuracy: 0.9818\n",
      "Epoch 120/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0323 - accuracy: 0.9877 - val_loss: 0.0584 - val_accuracy: 0.9794\n",
      "Epoch 121/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0348 - accuracy: 0.9879 - val_loss: 0.0653 - val_accuracy: 0.9788\n",
      "Epoch 122/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0541 - accuracy: 0.9812 - val_loss: 0.0704 - val_accuracy: 0.9757\n",
      "Epoch 123/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0571 - accuracy: 0.9771 - val_loss: 0.1923 - val_accuracy: 0.8768\n",
      "Epoch 124/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0666 - accuracy: 0.9750 - val_loss: 0.0560 - val_accuracy: 0.9769\n",
      "Epoch 125/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0345 - accuracy: 0.9853 - val_loss: 0.0836 - val_accuracy: 0.9648\n",
      "Epoch 126/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0367 - accuracy: 0.9853 - val_loss: 0.0687 - val_accuracy: 0.9745\n",
      "Epoch 127/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0341 - accuracy: 0.9870 - val_loss: 0.0541 - val_accuracy: 0.9788\n",
      "Epoch 128/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0360 - accuracy: 0.9848 - val_loss: 0.0635 - val_accuracy: 0.9806\n",
      "Epoch 129/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0349 - accuracy: 0.9863 - val_loss: 0.0540 - val_accuracy: 0.9800\n",
      "Epoch 130/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0350 - accuracy: 0.9873 - val_loss: 0.0851 - val_accuracy: 0.9678\n",
      "Epoch 131/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0340 - accuracy: 0.9874 - val_loss: 0.0541 - val_accuracy: 0.9806\n",
      "Epoch 132/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0346 - accuracy: 0.9860 - val_loss: 0.0602 - val_accuracy: 0.9775\n",
      "Epoch 133/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0342 - accuracy: 0.9879 - val_loss: 0.1093 - val_accuracy: 0.9618\n",
      "Epoch 134/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0376 - accuracy: 0.9850 - val_loss: 0.0587 - val_accuracy: 0.9806\n",
      "Epoch 135/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0330 - accuracy: 0.9880 - val_loss: 0.0630 - val_accuracy: 0.9788\n",
      "Epoch 136/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0353 - accuracy: 0.9859 - val_loss: 0.0705 - val_accuracy: 0.9763\n",
      "Epoch 137/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0364 - accuracy: 0.9862 - val_loss: 0.0638 - val_accuracy: 0.9782\n",
      "Epoch 138/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0364 - accuracy: 0.9856 - val_loss: 0.0561 - val_accuracy: 0.9812\n",
      "Epoch 139/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9806 - val_loss: 0.0580 - val_accuracy: 0.9800\n",
      "Epoch 140/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0507 - accuracy: 0.9826 - val_loss: 0.0573 - val_accuracy: 0.9818\n",
      "Epoch 141/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0314 - accuracy: 0.9883 - val_loss: 0.0553 - val_accuracy: 0.9806\n",
      "Epoch 142/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0325 - accuracy: 0.9866 - val_loss: 0.0620 - val_accuracy: 0.9806\n",
      "Epoch 143/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0344 - accuracy: 0.9860 - val_loss: 0.0639 - val_accuracy: 0.9806\n",
      "Epoch 144/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0316 - accuracy: 0.9877 - val_loss: 0.0565 - val_accuracy: 0.9818\n",
      "Epoch 145/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0334 - accuracy: 0.9863 - val_loss: 0.0557 - val_accuracy: 0.9818\n",
      "Epoch 146/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0692 - accuracy: 0.9832 - val_loss: 0.0564 - val_accuracy: 0.9806\n",
      "Epoch 147/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0855 - accuracy: 0.9639 - val_loss: 0.0595 - val_accuracy: 0.9794\n",
      "Epoch 148/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0311 - accuracy: 0.9883 - val_loss: 0.0561 - val_accuracy: 0.9818\n",
      "Epoch 149/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0594 - accuracy: 0.9809 - val_loss: 0.4137 - val_accuracy: 0.9072\n",
      "Epoch 150/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1162 - accuracy: 0.9700 - val_loss: 0.0771 - val_accuracy: 0.9715\n",
      "Epoch 151/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0405 - accuracy: 0.9853 - val_loss: 0.0672 - val_accuracy: 0.9794\n",
      "Epoch 152/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0697 - accuracy: 0.9766 - val_loss: 0.0672 - val_accuracy: 0.9763\n",
      "Epoch 153/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0331 - accuracy: 0.9873 - val_loss: 0.0698 - val_accuracy: 0.9763\n",
      "Epoch 154/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0361 - accuracy: 0.9874 - val_loss: 0.0608 - val_accuracy: 0.9775\n",
      "Epoch 155/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0320 - accuracy: 0.9871 - val_loss: 0.0611 - val_accuracy: 0.9794\n",
      "Epoch 156/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0319 - accuracy: 0.9885 - val_loss: 0.0606 - val_accuracy: 0.9788\n",
      "Epoch 157/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0515 - accuracy: 0.9819 - val_loss: 0.0639 - val_accuracy: 0.9782\n",
      "Epoch 158/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0291 - accuracy: 0.9895 - val_loss: 0.0572 - val_accuracy: 0.9800\n",
      "Epoch 159/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0531 - accuracy: 0.9800 - val_loss: 0.0585 - val_accuracy: 0.9800\n",
      "Epoch 160/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0652 - accuracy: 0.9745 - val_loss: 0.1510 - val_accuracy: 0.9551\n",
      "Epoch 161/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0368 - accuracy: 0.9851 - val_loss: 0.0621 - val_accuracy: 0.9769\n",
      "Epoch 162/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0319 - accuracy: 0.9865 - val_loss: 0.0645 - val_accuracy: 0.9788\n",
      "Epoch 163/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0365 - accuracy: 0.9863 - val_loss: 0.0592 - val_accuracy: 0.9800\n",
      "Epoch 164/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0452 - accuracy: 0.9822 - val_loss: 0.0628 - val_accuracy: 0.9800\n",
      "Epoch 165/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0315 - accuracy: 0.9880 - val_loss: 0.0579 - val_accuracy: 0.9794\n",
      "Epoch 166/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0313 - accuracy: 0.9888 - val_loss: 0.0570 - val_accuracy: 0.9788\n",
      "Epoch 167/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0301 - accuracy: 0.9891 - val_loss: 0.0587 - val_accuracy: 0.9788\n",
      "Epoch 168/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0579 - accuracy: 0.9819 - val_loss: 0.0606 - val_accuracy: 0.9800\n",
      "Epoch 169/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0315 - accuracy: 0.9876 - val_loss: 0.0589 - val_accuracy: 0.9794\n",
      "Epoch 170/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0321 - accuracy: 0.9879 - val_loss: 0.0894 - val_accuracy: 0.9654\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0336 - accuracy: 0.9866 - val_loss: 0.0568 - val_accuracy: 0.9800\n",
      "Epoch 172/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0314 - accuracy: 0.9886 - val_loss: 0.0848 - val_accuracy: 0.9715\n",
      "Epoch 173/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0306 - accuracy: 0.9879 - val_loss: 0.0561 - val_accuracy: 0.9800\n",
      "Epoch 174/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0306 - accuracy: 0.9879 - val_loss: 0.0576 - val_accuracy: 0.9812\n",
      "Epoch 175/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0319 - accuracy: 0.9886 - val_loss: 0.0552 - val_accuracy: 0.9800\n",
      "Epoch 176/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0345 - accuracy: 0.9871 - val_loss: 0.0563 - val_accuracy: 0.9800\n",
      "Epoch 177/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0318 - accuracy: 0.9879 - val_loss: 0.0583 - val_accuracy: 0.9788\n",
      "Epoch 178/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0294 - accuracy: 0.9882 - val_loss: 0.1208 - val_accuracy: 0.9575\n",
      "Epoch 179/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0640 - accuracy: 0.9777 - val_loss: 0.0625 - val_accuracy: 0.9806\n",
      "Epoch 180/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0315 - accuracy: 0.9883 - val_loss: 0.0598 - val_accuracy: 0.9824\n",
      "Epoch 181/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0334 - accuracy: 0.9880 - val_loss: 0.0688 - val_accuracy: 0.9769\n",
      "Epoch 182/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1557 - accuracy: 0.9657 - val_loss: 0.0715 - val_accuracy: 0.9788\n",
      "Epoch 183/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0510 - accuracy: 0.9832 - val_loss: 0.0725 - val_accuracy: 0.9769\n",
      "Epoch 184/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9842 - val_loss: 0.0651 - val_accuracy: 0.9769\n",
      "Epoch 185/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0377 - accuracy: 0.9877 - val_loss: 0.0656 - val_accuracy: 0.9775\n",
      "Epoch 186/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0372 - accuracy: 0.9851 - val_loss: 0.0652 - val_accuracy: 0.9788\n",
      "Epoch 187/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0356 - accuracy: 0.9851 - val_loss: 0.0694 - val_accuracy: 0.9745\n",
      "Epoch 188/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0430 - accuracy: 0.9870 - val_loss: 0.1097 - val_accuracy: 0.9417\n",
      "Epoch 189/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0360 - accuracy: 0.9862 - val_loss: 0.0610 - val_accuracy: 0.9812\n",
      "Epoch 190/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0306 - accuracy: 0.9898 - val_loss: 0.0563 - val_accuracy: 0.9806\n",
      "Epoch 191/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0614 - accuracy: 0.9777 - val_loss: 0.0571 - val_accuracy: 0.9800\n",
      "Epoch 192/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0551 - accuracy: 0.9824 - val_loss: 0.0587 - val_accuracy: 0.9794\n",
      "Epoch 193/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0316 - accuracy: 0.9891 - val_loss: 0.0565 - val_accuracy: 0.9788\n",
      "Epoch 194/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0675 - accuracy: 0.9800 - val_loss: 0.0884 - val_accuracy: 0.9721\n",
      "Epoch 195/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0304 - accuracy: 0.9883 - val_loss: 0.0601 - val_accuracy: 0.9812\n",
      "Epoch 196/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0680 - accuracy: 0.9809 - val_loss: 0.0803 - val_accuracy: 0.9757\n",
      "Epoch 197/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0690 - accuracy: 0.9733 - val_loss: 0.0624 - val_accuracy: 0.9824\n",
      "Epoch 198/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0300 - accuracy: 0.9876 - val_loss: 0.0601 - val_accuracy: 0.9788\n",
      "Epoch 199/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0294 - accuracy: 0.9882 - val_loss: 0.0578 - val_accuracy: 0.9812\n",
      "Epoch 200/200\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0272 - accuracy: 0.9901 - val_loss: 0.0637 - val_accuracy: 0.9794\n",
      "Best epoch: 96\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, Y_train, epochs=200, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una nueva instancia del hipermodelo y lo entrenamos con el número óptimo de épocas que hemos obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "Epoch 1/96\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.4518 - accuracy: 0.8229 - val_loss: 0.1884 - val_accuracy: 0.9205\n",
      "Epoch 2/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1865 - accuracy: 0.9264 - val_loss: 0.1020 - val_accuracy: 0.9612\n",
      "Epoch 3/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1303 - accuracy: 0.9484 - val_loss: 0.1122 - val_accuracy: 0.9569\n",
      "Epoch 4/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1596 - accuracy: 0.9416 - val_loss: 0.1809 - val_accuracy: 0.9260\n",
      "Epoch 5/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1128 - accuracy: 0.9592 - val_loss: 0.0976 - val_accuracy: 0.9569\n",
      "Epoch 6/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1237 - accuracy: 0.9496 - val_loss: 0.1574 - val_accuracy: 0.9254\n",
      "Epoch 7/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0890 - accuracy: 0.9639 - val_loss: 0.1251 - val_accuracy: 0.9399\n",
      "Epoch 8/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1064 - accuracy: 0.9596 - val_loss: 0.0685 - val_accuracy: 0.9709\n",
      "Epoch 9/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1108 - accuracy: 0.9587 - val_loss: 0.0952 - val_accuracy: 0.9587\n",
      "Epoch 10/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1080 - accuracy: 0.9604 - val_loss: 0.1275 - val_accuracy: 0.9612\n",
      "Epoch 11/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1534 - accuracy: 0.9454 - val_loss: 0.0925 - val_accuracy: 0.9600\n",
      "Epoch 12/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0724 - accuracy: 0.9715 - val_loss: 0.0621 - val_accuracy: 0.9727\n",
      "Epoch 13/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0879 - accuracy: 0.9678 - val_loss: 0.1041 - val_accuracy: 0.9551\n",
      "Epoch 14/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0687 - accuracy: 0.9733 - val_loss: 0.0620 - val_accuracy: 0.9751\n",
      "Epoch 15/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0708 - accuracy: 0.9738 - val_loss: 0.0579 - val_accuracy: 0.9782\n",
      "Epoch 16/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0723 - accuracy: 0.9706 - val_loss: 0.0627 - val_accuracy: 0.9745\n",
      "Epoch 17/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0985 - accuracy: 0.9663 - val_loss: 0.0752 - val_accuracy: 0.9721\n",
      "Epoch 18/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0761 - accuracy: 0.9733 - val_loss: 0.1586 - val_accuracy: 0.9387\n",
      "Epoch 19/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0776 - accuracy: 0.9701 - val_loss: 0.0630 - val_accuracy: 0.9751\n",
      "Epoch 20/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0881 - accuracy: 0.9674 - val_loss: 0.0874 - val_accuracy: 0.9624\n",
      "Epoch 21/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0531 - accuracy: 0.9788 - val_loss: 0.0879 - val_accuracy: 0.9630\n",
      "Epoch 22/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0694 - accuracy: 0.9719 - val_loss: 0.0745 - val_accuracy: 0.9727\n",
      "Epoch 23/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1144 - accuracy: 0.9601 - val_loss: 0.0683 - val_accuracy: 0.9757\n",
      "Epoch 24/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0599 - accuracy: 0.9756 - val_loss: 0.0589 - val_accuracy: 0.9763\n",
      "Epoch 25/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0799 - accuracy: 0.9686 - val_loss: 0.0608 - val_accuracy: 0.9788\n",
      "Epoch 26/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0480 - accuracy: 0.9816 - val_loss: 0.0680 - val_accuracy: 0.9745\n",
      "Epoch 27/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0566 - accuracy: 0.9772 - val_loss: 0.0540 - val_accuracy: 0.9806\n",
      "Epoch 28/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0585 - accuracy: 0.9782 - val_loss: 0.1452 - val_accuracy: 0.9587\n",
      "Epoch 29/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0652 - accuracy: 0.9751 - val_loss: 0.1239 - val_accuracy: 0.9606\n",
      "Epoch 30/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0761 - accuracy: 0.9731 - val_loss: 0.0585 - val_accuracy: 0.9775\n",
      "Epoch 31/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0574 - accuracy: 0.9789 - val_loss: 0.0543 - val_accuracy: 0.9788\n",
      "Epoch 32/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0618 - accuracy: 0.9765 - val_loss: 0.0563 - val_accuracy: 0.9812\n",
      "Epoch 33/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0980 - accuracy: 0.9678 - val_loss: 0.0589 - val_accuracy: 0.9775\n",
      "Epoch 34/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0543 - accuracy: 0.9786 - val_loss: 0.0993 - val_accuracy: 0.9606\n",
      "Epoch 35/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0497 - accuracy: 0.9810 - val_loss: 0.0548 - val_accuracy: 0.9800\n",
      "Epoch 36/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0493 - accuracy: 0.9813 - val_loss: 0.0520 - val_accuracy: 0.9788\n",
      "Epoch 37/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0644 - accuracy: 0.9760 - val_loss: 0.1721 - val_accuracy: 0.9411\n",
      "Epoch 38/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0529 - accuracy: 0.9794 - val_loss: 0.0547 - val_accuracy: 0.9794\n",
      "Epoch 39/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0443 - accuracy: 0.9813 - val_loss: 0.0609 - val_accuracy: 0.9751\n",
      "Epoch 40/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0481 - accuracy: 0.9813 - val_loss: 0.0530 - val_accuracy: 0.9788\n",
      "Epoch 41/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0496 - accuracy: 0.9827 - val_loss: 0.0654 - val_accuracy: 0.9775\n",
      "Epoch 42/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0771 - accuracy: 0.9727 - val_loss: 0.0545 - val_accuracy: 0.9782\n",
      "Epoch 43/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0437 - accuracy: 0.9827 - val_loss: 0.0951 - val_accuracy: 0.9563\n",
      "Epoch 44/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0496 - accuracy: 0.9815 - val_loss: 0.0787 - val_accuracy: 0.9727\n",
      "Epoch 45/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0944 - accuracy: 0.9695 - val_loss: 0.0997 - val_accuracy: 0.9636\n",
      "Epoch 46/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0516 - accuracy: 0.9794 - val_loss: 0.0769 - val_accuracy: 0.9739\n",
      "Epoch 47/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0504 - accuracy: 0.9801 - val_loss: 0.0569 - val_accuracy: 0.9794\n",
      "Epoch 48/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0453 - accuracy: 0.9830 - val_loss: 0.1039 - val_accuracy: 0.9703\n",
      "Epoch 49/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0444 - accuracy: 0.9819 - val_loss: 0.0522 - val_accuracy: 0.9794\n",
      "Epoch 50/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0440 - accuracy: 0.9822 - val_loss: 0.0667 - val_accuracy: 0.9757\n",
      "Epoch 51/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0431 - accuracy: 0.9839 - val_loss: 0.0769 - val_accuracy: 0.9733\n",
      "Epoch 52/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0770 - accuracy: 0.9725 - val_loss: 0.0553 - val_accuracy: 0.9812\n",
      "Epoch 53/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0437 - accuracy: 0.9815 - val_loss: 0.0585 - val_accuracy: 0.9782\n",
      "Epoch 54/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.2992 - accuracy: 0.9571 - val_loss: 1.3621 - val_accuracy: 0.8004\n",
      "Epoch 55/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0936 - accuracy: 0.9695 - val_loss: 0.0635 - val_accuracy: 0.9788\n",
      "Epoch 56/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0701 - accuracy: 0.9757 - val_loss: 0.0593 - val_accuracy: 0.9775\n",
      "Epoch 57/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0495 - accuracy: 0.9804 - val_loss: 0.0794 - val_accuracy: 0.9691\n",
      "Epoch 58/96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0411 - accuracy: 0.9829 - val_loss: 0.0496 - val_accuracy: 0.9812\n",
      "Epoch 59/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0386 - accuracy: 0.9845 - val_loss: 0.0550 - val_accuracy: 0.9788\n",
      "Epoch 60/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0420 - accuracy: 0.9832 - val_loss: 0.0508 - val_accuracy: 0.9818\n",
      "Epoch 61/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0440 - accuracy: 0.9827 - val_loss: 0.0584 - val_accuracy: 0.9769\n",
      "Epoch 62/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0381 - accuracy: 0.9863 - val_loss: 0.0724 - val_accuracy: 0.9715\n",
      "Epoch 63/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0416 - accuracy: 0.9839 - val_loss: 0.0564 - val_accuracy: 0.9806\n",
      "Epoch 64/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0451 - accuracy: 0.9832 - val_loss: 0.0562 - val_accuracy: 0.9788\n",
      "Epoch 65/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0383 - accuracy: 0.9863 - val_loss: 0.0616 - val_accuracy: 0.9788\n",
      "Epoch 66/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0406 - accuracy: 0.9847 - val_loss: 0.0586 - val_accuracy: 0.9733\n",
      "Epoch 67/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.1561 - accuracy: 0.9583 - val_loss: 0.1086 - val_accuracy: 0.9697\n",
      "Epoch 68/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0695 - accuracy: 0.9757 - val_loss: 0.1361 - val_accuracy: 0.9527\n",
      "Epoch 69/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0451 - accuracy: 0.9813 - val_loss: 0.0494 - val_accuracy: 0.9830\n",
      "Epoch 70/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0395 - accuracy: 0.9842 - val_loss: 0.0538 - val_accuracy: 0.9788\n",
      "Epoch 71/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0395 - accuracy: 0.9842 - val_loss: 0.0661 - val_accuracy: 0.9745\n",
      "Epoch 72/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0481 - accuracy: 0.9821 - val_loss: 0.0611 - val_accuracy: 0.9775\n",
      "Epoch 73/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0417 - accuracy: 0.9836 - val_loss: 0.1344 - val_accuracy: 0.9569\n",
      "Epoch 74/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9804 - val_loss: 0.0658 - val_accuracy: 0.9745\n",
      "Epoch 75/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0440 - accuracy: 0.9822 - val_loss: 0.0489 - val_accuracy: 0.9830\n",
      "Epoch 76/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0418 - accuracy: 0.9830 - val_loss: 0.0469 - val_accuracy: 0.9830\n",
      "Epoch 77/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0454 - accuracy: 0.9819 - val_loss: 0.0727 - val_accuracy: 0.9751\n",
      "Epoch 78/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0449 - accuracy: 0.9836 - val_loss: 0.0515 - val_accuracy: 0.9830\n",
      "Epoch 79/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0383 - accuracy: 0.9841 - val_loss: 0.0550 - val_accuracy: 0.9782\n",
      "Epoch 80/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0382 - accuracy: 0.9835 - val_loss: 0.0638 - val_accuracy: 0.9751\n",
      "Epoch 81/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0412 - accuracy: 0.9835 - val_loss: 0.0643 - val_accuracy: 0.9763\n",
      "Epoch 82/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0833 - accuracy: 0.9713 - val_loss: 0.0582 - val_accuracy: 0.9782\n",
      "Epoch 83/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0374 - accuracy: 0.9839 - val_loss: 0.0601 - val_accuracy: 0.9775\n",
      "Epoch 84/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0353 - accuracy: 0.9860 - val_loss: 0.0571 - val_accuracy: 0.9806\n",
      "Epoch 85/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9833 - val_loss: 0.0973 - val_accuracy: 0.9672\n",
      "Epoch 86/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0485 - accuracy: 0.9824 - val_loss: 0.0729 - val_accuracy: 0.9745\n",
      "Epoch 87/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0699 - accuracy: 0.9762 - val_loss: 0.0715 - val_accuracy: 0.9733\n",
      "Epoch 88/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0385 - accuracy: 0.9842 - val_loss: 0.0994 - val_accuracy: 0.9636\n",
      "Epoch 89/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0622 - accuracy: 0.9780 - val_loss: 0.0868 - val_accuracy: 0.9642\n",
      "Epoch 90/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0494 - accuracy: 0.9812 - val_loss: 0.0587 - val_accuracy: 0.9788\n",
      "Epoch 91/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0377 - accuracy: 0.9842 - val_loss: 0.1349 - val_accuracy: 0.9521\n",
      "Epoch 92/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0534 - accuracy: 0.9810 - val_loss: 0.0524 - val_accuracy: 0.9800\n",
      "Epoch 93/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0408 - accuracy: 0.9829 - val_loss: 0.0611 - val_accuracy: 0.9788\n",
      "Epoch 94/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0333 - accuracy: 0.9871 - val_loss: 0.0502 - val_accuracy: 0.9788\n",
      "Epoch 95/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0346 - accuracy: 0.9841 - val_loss: 0.0527 - val_accuracy: 0.9818\n",
      "Epoch 96/96\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0309 - accuracy: 0.9880 - val_loss: 0.0465 - val_accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x260ae12f4c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(X_train, Y_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEw4bZgGCaXB"
   },
   "source": [
    "## Evaluar Precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0407 - accuracy: 0.9854\n",
      "[test loss, test accuracy]: [0.04072854295372963, 0.9854369163513184]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(X_validation, Y_validation)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T00:12:36.459549Z",
     "iopub.status.busy": "2020-09-23T00:12:36.458790Z",
     "iopub.status.idle": "2020-09-23T00:12:37.082116Z",
     "shell.execute_reply": "2020-09-23T00:12:37.082547Z"
    },
    "id": "VflXLEeECaXC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.9888336062431335% \n",
      " Error on training data: 0.011166393756866455\n",
      "Accuracy on test data: 0.9854369163513184% \n",
      " Error on test data: 0.01456308364868164\n"
     ]
    }
   ],
   "source": [
    "pred_train= hypermodel.predict(X_train)\n",
    "scores = hypermodel.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Accuracy on training data: {}% \\n Error on training data: {}'.format(scores[1], 1 - scores[1]))   \n",
    " \n",
    "pred_test= hypermodel.predict(X_validation)\n",
    "scores2 = hypermodel.evaluate(X_validation, Y_validation, verbose=0)\n",
    "print('Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2[1], 1 - scores2[1]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo alcanza una precisión de 0.985 sobre el set de datos de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "En el modelo que utiliza Random Forest obtenemos unos resultados muy buenos, una precisión del 0.976 y no existe ningún desbalanceo de la predicción entre clases.\n",
    "\n",
    "Para el modelo clasificador basado en redes neuronales de TensorFlow los resultados son iguales e incluso un poco mejores, obteniendo una precisión del 0.985 en la clasificación.\n",
    "\n",
    "Para estos datos puede considerarse que el modelo basado en redes neuronales es más efectivo en cuanto a precisión de clasificación, aunque la diferencia es mínima, los 2 tienen una gran efectividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAE/CAYAAACNa1CbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcjklEQVR4nO3df7DddX3n8efLpKC46xI1ZZAgCTYqMWujXjOtXZ3diY7QdkxlbQ2jq6UwmB3BVcdpI53dOnacYatOy4xoJrZ0ZVehKM02bWnRsrOyu4MmFxIowaSGBCVA6W0dzbYw0Jt97x/nE/v1csM9CVfzueH5mDmT8/38+n4+wxxe+XzPN9+TqkKSJPXlWSd6ApIk6ckMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUOLT/QE5sMLX/jCWr58+YmehiRJx+SOO+7426paOlvdSRHQy5cvZ3Jy8kRPQ5KkY5LkW0er8xK3JEkdMqAlSeqQAS1JUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnqkAEtSVKHDGhJkjpkQEuS1CEDWpKkDhnQkiR1yICWJKlDBrQkSR0yoCVJ6tDiEz0BSc9Myzf96YmegnRc7r/q534k53EHLUlSh8YK6CTnJ9mbZF+STbPUL0myNcndSbYnWT2o+0CS3UnuSXJ9kme38o8keTDJrvb62UGfD7dz7U3y5vlYqCRJC8mcAZ1kEXANcAGwCrgoyaoZza4EdlXVK4F3AVe3vmcB7wMmqmo1sAjYMOj321W1pr1ubn1WtTavAM4HPt3mIEnSM8Y4O+i1wL6q2l9VTwA3AOtntFkF3ApQVXuA5UnOaHWLgeckWQycBjw0x/nWAzdU1eNVdQDY1+YgSdIzxjgBfRbwwOD4YCsbugu4ECDJWuAcYFlVPQh8Avg28DDwvar68qDf5e2y+LVJlhzD+SRJOqmNE9CZpaxmHF8FLEmyC7gC2AlMt9BdD6wAXgQ8N8k7W5/PAC8B1jAK708ew/lIclmSySSTU1NTYyxDkqSFY5yAPgicPThexozL1FV1qKourqo1jL6DXgocAN4IHKiqqar6R+APgde1Po9U1eGq+n/AZ/mny9hznq/131JVE1U1sXTp0jGWIUnSwjFOQO8AViZZkeQURjdwbRs2SHJ6qwO4FLitqg4xurT9U0lOSxJgHfCN1ufMwRBvBe5p77cBG5KcmmQFsBLYfnzLkyRpYZrzQSVVNZ3kcuAWRndhX1tVu5NsbPWbgfOA65IcBu4FLml1X0/yJeBOYJrRpe8tbejfSrKG0eXr+4H3tD67k9zYxpkG3ltVh+dnuZIkLQypetLXuwvOxMRETU5OnuhpSDoGPklMC9V8PkksyR1VNTFbnU8SkySpQwa0JEkdMqAlSeqQAS1JUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnqkAEtSVKHDGhJkjpkQEuS1CEDWpKkDhnQkiR1yICWJKlDBrQkSR0yoCVJ6pABLUlShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdWisgE5yfpK9SfYl2TRL/ZIkW5PcnWR7ktWDug8k2Z3kniTXJ3l2K/94kj2tz9Ykp7fy5UkeS7KrvTbP01olSVow5gzoJIuAa4ALgFXARUlWzWh2JbCrql4JvAu4uvU9C3gfMFFVq4FFwIbW5yvA6tbnr4APD8a7r6rWtNfG416dJEkL1Dg76LXAvqraX1VPADcA62e0WQXcClBVe4DlSc5odYuB5yRZDJwGPNTafbmqplubrwHLntZKJEk6iYwT0GcBDwyOD7ayobuACwGSrAXOAZZV1YPAJ4BvAw8D36uqL89yjl8B/mxwvCLJziRfTfL6sVYiSdJJZJyAzixlNeP4KmBJkl3AFcBOYDrJEka77RXAi4DnJnnnDwye/DowDXy+FT0MvLiqXgV8EPhCkuc9aVLJZUkmk0xOTU2NsQxJkhaOcQL6IHD24HgZ7TL1EVV1qKourqo1jL6DXgocAN4IHKiqqar6R+APgdcd6Zfk3cDPA++oqmpjPV5Vf9fe3wHcB7x05qSqaktVTVTVxNKlS8ddryRJC8I4Ab0DWJlkRZJTGN3ktW3YIMnprQ7gUuC2qjrE6NL2TyU5LUmAdcA3Wp/zgV8D3lJVjw7GWtpuTCPJucBKYP/TWaQkSQvN4rkaVNV0ksuBWxjdhX1tVe1OsrHVbwbOA65Lchi4F7ik1X09yZeAOxldxt4JbGlDfwo4FfjKKLv5Wrtj+w3AR5NMA4eBjVX1nflasCRJC8GcAQ1QVTcDN88o2zx4fzujne5sfX8D+I1Zyn/iKO1vAm4aZ16SJJ2sfJKYJEkdMqAlSeqQAS1JUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnqkAEtSVKHDGhJkjpkQEuS1CEDWpKkDhnQkiR1yICWJKlDBrQkSR0yoCVJ6pABLUlShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdciAliSpQ2MFdJLzk+xNsi/JplnqlyTZmuTuJNuTrB7UfSDJ7iT3JLk+ybNb+fOTfCXJN9ufSwZ9PtzOtTfJm+djoZIkLSRzBnSSRcA1wAXAKuCiJKtmNLsS2FVVrwTeBVzd+p4FvA+YqKrVwCJgQ+uzCbi1qlYCt7Zj2tgbgFcA5wOfbnOQJOkZY5wd9FpgX1Xtr6ongBuA9TParGIUslTVHmB5kjNa3WLgOUkWA6cBD7Xy9cDn2vvPAb8wKL+hqh6vqgPAvjYHSZKeMcYJ6LOABwbHB1vZ0F3AhQBJ1gLnAMuq6kHgE8C3gYeB71XVl1ufM6rqYYD2548fw/lIclmSySSTU1NTYyxDkqSFY5yAzixlNeP4KmBJkl3AFcBOYLp9r7weWAG8CHhuknfOw/moqi1VNVFVE0uXLp1jSEmSFpbFY7Q5CJw9OF7GP12mBqCqDgEXAyQJcKC93gwcqKqpVveHwOuA/wY8kuTMqno4yZnA34x7PkmSTnbj7KB3ACuTrEhyCqMbuLYNGyQ5vdUBXArc1kL728BPJTmtBfc64But3Tbg3e39u4E/GpRvSHJqkhXASmD78S1PkqSFac4ddFVNJ7kcuIXRXdjXVtXuJBtb/WbgPOC6JIeBe4FLWt3Xk3wJuBOYZnTpe0sb+irgxiSXMAryX2x9die5sY0zDby3qg7P14IlSVoIUvWkr3cXnImJiZqcnDzR05B0DJZv+tMTPQXpuNx/1c/N21hJ7qiqidnqfJKYJEkdMqAlSeqQAS1JUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnqkAEtSVKHDGhJkjpkQEuS1CEDWpKkDhnQkiR1yICWJKlDBrQkSR0yoCVJ6pABLUlShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdciAliSpQ2MFdJLzk+xNsi/JplnqlyTZmuTuJNuTrG7lL0uya/A6lOT9re4PBuX3J9nVypcneWxQt3n+litJ0sKweK4GSRYB1wBvAg4CO5Jsq6p7B82uBHZV1VuTvLy1X1dVe4E1g3EeBLYCVNXbB+f4JPC9wXj3VdWap7EuSZIWtHF20GuBfVW1v6qeAG4A1s9oswq4FaCq9gDLk5wxo806RsH7rWFhkgC/BFx/HPOXJOmkNE5AnwU8MDg+2MqG7gIuBEiyFjgHWDajzQZmD+HXA49U1TcHZSuS7Ezy1SSvn21SSS5LMplkcmpqaoxlSJK0cIwT0JmlrGYcXwUsad8jXwHsBKa/P0ByCvAW4IuzjHURPxjcDwMvrqpXAR8EvpDkeU+aQNWWqpqoqomlS5eOsQxJkhaOOb+DZrRjPntwvAx4aNigqg4BF8P3L1kfaK8jLgDurKpHhv2SLGa0837NYKzHgcfb+zuS3Ae8FJgcb0mSJC184+ygdwArk6xoO+ENwLZhgySntzqAS4HbWmgfMXOXfMQbgT1VdXAw1tJ2QxlJzgVWAvvHXZAkSSeDOXfQVTWd5HLgFmARcG1V7U6ysdVvBs4DrktyGLgXuORI/ySnMboD/D2zDD/b99JvAD6aZBo4DGysqu8c88okSVrAxrnETVXdDNw8o2zz4P3tjHa6s/V9FHjBUep+eZaym4CbxpmXJEknK58kJklShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdciAliSpQwa0JEkdMqAlSeqQAS1JUofGepLYM83yTX96oqcgHZf7r/q5Ez0FSfPEHbQkSR0yoCVJ6pABLUlShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdciAliSpQwa0JEkdMqAlSeqQAS1JUocMaEmSOmRAS5LUobECOsn5SfYm2Zdk0yz1S5JsTXJ3ku1JVrfylyXZNXgdSvL+VveRJA8O6n52MN6H27n2JnnzPK1VkqQFY86fm0yyCLgGeBNwENiRZFtV3TtodiWwq6remuTlrf26qtoLrBmM8yCwddDvt6vqEzPOtwrYALwCeBHwF0leWlWHj3ONkiQtOOPsoNcC+6pqf1U9AdwArJ/RZhVwK0BV7QGWJzljRpt1wH1V9a05zrceuKGqHq+qA8C+NgdJkp4xxgnos4AHBscHW9nQXcCFAEnWAucAy2a02QBcP6Ps8nZZ/NokS47hfJIkndTGCejMUlYzjq8CliTZBVwB7ASmvz9AcgrwFuCLgz6fAV7C6BL4w8Anj+F8JLksyWSSyampqTGWIUnSwjHnd9CMdrBnD46XAQ8NG1TVIeBigCQBDrTXERcAd1bVI4M+33+f5LPAn4x7vtZ/C7AFYGJi4kkBLknSQjbODnoHsDLJirYT3gBsGzZIcnqrA7gUuK2F9hEXMePydpIzB4dvBe5p77cBG5KcmmQFsBLYPu6CJEk6Gcy5g66q6SSXA7cAi4Brq2p3ko2tfjNwHnBdksPAvcAlR/onOY3RHeDvmTH0byVZw+jy9f1H6tvYN7ZxpoH3ege3JOmZZpxL3FTVzcDNM8o2D97fzminO1vfR4EXzFL+757ifB8DPjbO3CRJOhn5JDFJkjpkQEuS1CEDWpKkDhnQkiR1yICWJKlDBrQkSR0yoCVJ6pABLUlShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdciAliSpQwa0JEkdMqAlSeqQAS1JUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnqkAEtSVKHxgroJOcn2ZtkX5JNs9QvSbI1yd1JtidZ3cpflmTX4HUoyftb3ceT7Gl9tiY5vZUvT/LYoM/m+VuuJEkLw5wBnWQRcA1wAbAKuCjJqhnNrgR2VdUrgXcBVwNU1d6qWlNVa4DXAI8CW1ufrwCrW5+/Aj48GO++I/2qauNxr06SpAVqnB30WmBfVe2vqieAG4D1M9qsAm4FqKo9wPIkZ8xos45R8H6rtftyVU23uq8By45zDZIknXTGCeizgAcGxwdb2dBdwIUASdYC5/DkwN0AXH+Uc/wK8GeD4xVJdib5apLXjzFHSZJOKuMEdGYpqxnHVwFLkuwCrgB2Akd2xyQ5BXgL8MUnDZ78emv7+Vb0MPDiqnoV8EHgC0meN0u/y5JMJpmcmpoaYxmSJC0ci8docxA4e3C8DHho2KCqDgEXAyQJcKC9jrgAuLOqHhn2S/Ju4OeBdVVVbazHgcfb+zuS3Ae8FJiccc4twBaAiYmJmX9hkCRpQRtnB70DWJlkRdsJbwC2DRskOb3VAVwK3NZC+4iLmHF5O8n5wK8Bb6mqRwflS9uNaSQ5F1gJ7D+2ZUmStLDNuYOuqukklwO3AIuAa6tqd5KNrX4zcB5wXZLDwL3AJUf6JzkNeBPwnhlDfwo4FfjKaNPN19od228APppkGjgMbKyq7zy9ZUqStLCMc4mbqroZuHlG2ebB+9sZ7XRn6/so8IJZyn/iKO1vAm4aZ16SJJ2sfJKYJEkdMqAlSeqQAS1JUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnqkAEtSVKHDGhJkjpkQEuS1CEDWpKkDhnQkiR1yICWJKlDBrQkSR0yoCVJ6pABLUlShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdciAliSpQ2MFdJLzk+xNsi/JplnqlyTZmuTuJNuTrG7lL0uya/A6lOT9re75Sb6S5JvtzyWD8T7czrU3yZvnaa2SJC0YcwZ0kkXANcAFwCrgoiSrZjS7EthVVa8E3gVcDVBVe6tqTVWtAV4DPApsbX02AbdW1Urg1nZMG3sD8ArgfODTbQ6SJD1jjLODXgvsq6r9VfUEcAOwfkabVYxClqraAyxPcsaMNuuA+6rqW+14PfC59v5zwC8Mym+oqser6gCwr81BkqRnjHEC+izggcHxwVY2dBdwIUCStcA5wLIZbTYA1w+Oz6iqhwHanz9+DOcjyWVJJpNMTk1NjbEMSZIWjnECOrOU1Yzjq4AlSXYBVwA7genvD5CcArwF+OI8nY+q2lJVE1U1sXTp0jGGlSRp4Vg8RpuDwNmD42XAQ8MGVXUIuBggSYAD7XXEBcCdVfXIoOyRJGdW1cNJzgT+ZtzzSZJ0shtnB70DWJlkRdsJbwC2DRskOb3VAVwK3NZC+4iL+MHL27Qx3t3evxv4o0H5hiSnJlkBrAS2j7sgSZJOBnPuoKtqOsnlwC3AIuDaqtqdZGOr3wycB1yX5DBwL3DJkf5JTgPeBLxnxtBXATcmuQT4NvCLbbzdSW5s40wD762qw09vmZIkLSzjXOKmqm4Gbp5Rtnnw/nZGO93Z+j4KvGCW8r9jdGf3bH0+BnxsnLlJknQy8klikiR1yICWJKlDBrQkSR0yoCVJ6pABLUlShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdciAliSpQwa0JEkdMqAlSeqQAS1JUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnqkAEtSVKHDGhJkjpkQEuS1KGxAjrJ+Un2JtmXZNMs9UuSbE1yd5LtSVYP6k5P8qUke5J8I8lPt/I/SLKrve5PsquVL0/y2KBu8zytVZKkBWPxXA2SLAKuAd4EHAR2JNlWVfcOml0J7KqqtyZ5eWu/rtVdDfx5Vb0tySnAaQBV9fbBOT4JfG8w3n1Vteb4lyVJ0sI2zg56LbCvqvZX1RPADcD6GW1WAbcCVNUeYHmSM5I8D3gD8Hut7omq+u6wY5IAvwRc/3QWIknSyWScgD4LeGBwfLCVDd0FXAiQZC1wDrAMOBeYAn4/yc4kv5vkuTP6vh54pKq+OShb0dp/Ncnrx1+OJEknh3ECOrOU1Yzjq4Al7XvkK4CdwDSjS+ivBj5TVa8C/gGY+R32Rfzg7vlh4MWt/QeBL7Sd+A9OKrksyWSSyampqTGWIUnSwjFOQB8Ezh4cLwMeGjaoqkNVdXH73vhdwFLgQOt7sKq+3pp+iVFgA5BkMaOd9x8Mxnq8qv6uvb8DuA946cxJVdWWqpqoqomlS5eOsQxJkhaOcQJ6B7AyyYp2k9cGYNuwQbtT+5R2eClwWwvtvwYeSPKyVrcOGN5c9kZgT1UdHIy1tN2YRpJzgZXA/uNYmyRJC9acd3FX1XSSy4FbgEXAtVW1O8nGVr8ZOA+4LslhRgF8yWCIK4DPtwDfD1w8qNvAk28OewPw0STTwGFgY1V957hWJ0nSAjVnQANU1c3AzTPKNg/e385opztb313AxFHqfnmWspuAm8aZlyRJJyufJCZJUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnqkAEtSVKHDGhJkjpkQEuS1CEDWpKkDhnQkiR1yICWJKlDBrQkSR0yoCVJ6pABLUlShwxoSZI6ZEBLktQhA1qSpA4Z0JIkdciAliSpQwa0JEkdMqAlSerQWAGd5Pwke5PsS7JplvolSbYmuTvJ9iSrB3WnJ/lSkj1JvpHkp1v5R5I8mGRXe/3soM+H27n2JnnzfCxUkqSFZPFcDZIsAq4B3gQcBHYk2VZV9w6aXQnsqqq3Jnl5a7+u1V0N/HlVvS3JKcBpg36/XVWfmHG+VcAG4BXAi4C/SPLSqjp8fEuUJGnhGWcHvRbYV1X7q+oJ4AZg/Yw2q4BbAapqD7A8yRlJnge8Afi9VvdEVX13jvOtB26oqser6gCwr81BkqRnjHEC+izggcHxwVY2dBdwIUCStcA5wDLgXGAK+P0kO5P8bpLnDvpd3i6LX5tkyTGcT5Kkk9qcl7iBzFJWM46vAq5Osgv4S2AnMA38GPBq4Iqq+nqSq4FNwH8EPgP8ZhvrN4FPAr8y5vlIchlwWTv8+yR7x1iLTrwXAn97oidxssp/PtEzUCf8nP0QzfPn7JyjVYwT0AeBswfHy4CHhg2q6hBwMUCSAAfa6zTgYFV9vTX9EqOApqoeOdI/yWeBPxn3fK3/FmDLGPNXR5JMVtXEiZ6HdDLzc3ZyGOcS9w5gZZIV7SavDcC2YYN2p/Yp7fBS4LaqOlRVfw08kORlrW4dcG/rc+ZgiLcC97T324ANSU5NsgJYCWw/jrVJkrRgzbmDrqrpJJcDtwCLgGuraneSja1+M3AecF2Sw4wC+JLBEFcAn28Bvp+20wZ+K8kaRpev7wfe08bbneTGNs408F7v4JYkPdOk6klf70o/NEkua19PSPoh8XN2cjCgJUnqkI/6lCSpQwa05k2Sw+2xrfck+eMkp7fy5UkeGzzWddeRmwqTXJBksj0Gdk+STzzlSaQF4GifhWPo//dHKa8knxwcfyjJR57ebOdfkv+S5G0neh4LnQGt+fRYVa2pqtXAd4D3Durua3VHXk+0Z7Z/CnhnVZ0HrGZ0I6G00D3VZ+HpeBy4MMkL52k8YPTPY5OYB53xP4h+WG5n7ifA/SrwsfZ4WKpquqo+/UOfmfSj9f3PQpKXJPnzJHck+V/ttwto/4z19iQ7kvzmU4w1zej5Dx+YWZFkaZKb2hg7kvxMK/9Ikg8N2t3Trmotb1euPg3cCZyd5OOt/i+TvL21/9dJ/ufgR48+3553QZL/1M51T5ItR8o1Pwxozbv2Ayvr+MF/L/+SweXta1rZauCOH/kEpR+RWT4LWxg9WfE1wIeAI38hvRr4TFW9FvjrOYa9BnhHkn8xo/xqRj9A9Frg3wK/O8YUXwZcV1WvAiaANcBPAm8EPj54XsWrgPcz+t2Fc4GfaeWfqqrXtisFzwF+foxzakzjPElMGtdz2uNelzMK3q8M6u6rqjUnYE7SifCkz0KSfwa8DvjiYKN5avvzZxiFKsB/BY76MMmqOpTkOuB9wGODqjcCqwZjPy/JP59jnt+qqq+19/8KuL49d+KRJF8FXgscArZX1UGAwbr+N/Bvkvwqo6dGPh/YDfzxHOfUmNxBaz491kL4HOAU5v7ebTfwmh/2pKQTYLbPwrOA7864F+O8QZ9j+Tevv8PogVDDHx96FvDTg7HPqqr/y+iy+PD/9c8evP+Hwfunujz9+OD9YWBxkmczugLwtqr6l8BnZ4ytp8mA1ryrqu8x+tv9h5L82FM0/ThwZZKXAiR5VpIP/ijmKP0oDD8LjHa7B5L8Inz/xqyfbE3/D6PHKAO8Y4xxvwPcyA8+tfHLwOVHDtqTGmH0pMZXt7JXAyuOMuxtwNuTLEqylNFPBT/VY5aPhPHftqsD3rU9zwxo/VBU1U5GP0O64Sna3M3oe63rk3yD0fPYzzxae2khmvFZeAdwSZK7GF1BWt+a/QfgvUl2ADO/Wz6aTzL61aoj3gdMZPQTvvcCG1v5TcDz26Xpfw/81VHG2wrc3eb6P4Bfbb+ncLR1fZfRrvkvgf/O6HcbNI98kpgkSR1yBy1JUocMaEmSOmRAS5LUIQNakqQOGdCSJHXIgJYkqUMGtCRJHTKgJUnq0P8HKjZfduXkObcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "labels = ['RFC', 'Red Neuronal']\n",
    "modelos = [cv_results.mean(), eval_result[1]]\n",
    "ax.bar(labels,modelos)\n",
    "plt.ylim([0.965,0.987])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
